# AUTOGENERATED! DO NOT EDIT! File to edit: ../12_DM1.ipynb.

# %% auto 0
__all__ = ['BasicConvNet', 'PreNormResidual', 'FeedForward', 'MLPMixer', 'BasicUNet', 'NoiseConditionedUNet']

# %% ../12_DM1.ipynb 4
import torch
import torchvision
from torch import nn
from torch.nn import functional as F
from torch.utils.data import DataLoader
from matplotlib import pyplot as plt
from .data_utils import get_mnist_dl

# %% ../12_DM1.ipynb 18
#|code-fold: true
class BasicConvNet(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=5):
        super().__init__()
        padding = kernel_size // 2 # So we keep output size the same
        self.net = nn.Sequential(
            nn.Conv2d(in_channels, 16, kernel_size,  padding=padding),
            nn.ReLU(),
            nn.Conv2d(16, 32, kernel_size,  padding=padding),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size,  padding=padding),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size,  padding=padding),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size,  padding=padding),
            nn.ReLU(),
            nn.Conv2d(64, 16, kernel_size,  padding=padding),
            nn.ReLU(),
            nn.Conv2d(16, out_channels, kernel_size, padding=padding),
        )

    def forward(self, x):
        return self.net(x)

# %% ../12_DM1.ipynb 19
#|code-fold: true
from torch import nn
from functools import partial
from einops.layers.torch import Rearrange, Reduce

class PreNormResidual(nn.Module):
    def __init__(self, dim, fn):
        super().__init__()
        self.fn = fn
        self.norm = nn.LayerNorm(dim)

    def forward(self, x):
        return self.fn(self.norm(x)) + x

def FeedForward(dim, expansion_factor = 4, dropout = 0., dense = nn.Linear):
    inner_dim = int(dim * expansion_factor)
    return nn.Sequential(
        dense(dim, inner_dim),
        nn.GELU(),
        nn.Dropout(dropout),
        dense(inner_dim, dim),
        nn.Dropout(dropout)
    )


def MLPMixer(*, image_size, channels, patch_size, dim, depth, expansion_factor = 4, expansion_factor_token = 0.5, dropout = 0.):
    # Get image width and height (same if image_size isn't a tuple):
    pair = lambda x: x if isinstance(x, tuple) else (x, x)
    image_h, image_w = pair(image_size)
    # Check they divide neatly by patch_size
    assert (image_h % patch_size) == 0 and (image_w % patch_size) == 0, 'image must be divisible by patch size'
    num_patches = (image_h // patch_size) * (image_w // patch_size)
    # Prep the two layers
    chan_first, chan_last = partial(nn.Conv1d, kernel_size = 1), nn.Linear
    # Return the model (a stack of [FeedForward(chan_first), FeedForward(chan_last)] pairs
    # with layer norm on the inputs and a skip connection thanks to PreNormResidual)
    return nn.Sequential(
        Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_size, p2 = patch_size),
        nn.Linear((patch_size ** 2) * channels, dim),
        *[nn.Sequential(
            PreNormResidual(dim, FeedForward(num_patches, expansion_factor, dropout, chan_first)),
            PreNormResidual(dim, FeedForward(dim, expansion_factor_token, dropout, chan_last))
        ) for _ in range(depth)],
        Rearrange('b (h w) (p1 p2 c) -> b c (h p1) (w p2)', h = int(image_h/patch_size),  w = int(image_w/patch_size), p1 = patch_size, p2 = patch_size),
        nn.Conv2d(dim//(patch_size**2), channels, kernel_size=1) # Back to right number of channels
    )

# %% ../12_DM1.ipynb 20
#|code-fold: show
class BasicUNet(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        # Our list of layers, ina ModuleList so they show up as parameters
        self.down_layers = torch.nn.ModuleList([ 
            nn.Conv2d(in_channels, 32, kernel_size=5, padding=2),
            nn.Conv2d(32, 64, kernel_size=5, padding=2),
            nn.Conv2d(64, 64, kernel_size=5, padding=2),
        ])
        self.up_layers = torch.nn.ModuleList([
            nn.Conv2d(64, 64, kernel_size=5, padding=2),
            nn.Conv2d(64, 32, kernel_size=5, padding=2),
            nn.Conv2d(32, out_channels, kernel_size=5, padding=2), 
        ])
        self.act = nn.SiLU()
        self.downscale = nn.MaxPool2d(2)
        self.upscale = nn.Upsample(scale_factor=2)
        

    def forward(self, x):
        h = []
        for i, l in enumerate(self.down_layers):
            x = self.act(l(x))
            h.append(x)
            if i < 2: x = self.downscale(x)
        for i, l in enumerate(self.up_layers):
            if i > 0: x = self.upscale(x)
            x += h.pop()
            x = self.act(l(x))
        return x

# %% ../12_DM1.ipynb 37
#|code-fold: true
class NoiseConditionedUNet(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.net = BasicUNet(in_channels+1, out_channels)

    def forward(self, x, noise_amount):
        # Shape of x
        bs, ch, w, h = x.shape
        
        # Get noise_amount as a single channel 'image' the same shape as x
        if not torch.is_tensor(noise_amount):
            noise_amount = x.new_full((x.size(0),), noise_amount)
        noise_amount = noise_amount.view(-1, 1, 1, 1).expand(bs, 1, w, h) # If x.shape is [8,3,28,28] noise_amount is [8,1,28, 28]
        
        # Concatenate this onto x to get the final net input:
        net_input = torch.cat((x, noise_amount), 1)
        
        # Now pass through the net to get the prediction as before
        return self.net(net_input)

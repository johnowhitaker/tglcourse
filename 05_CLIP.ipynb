{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c751f886-c89a-4d2d-b176-807620f754e0",
   "metadata": {},
   "source": [
    "# Lesson 5: Exploring Multiple Modalities with CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becc9d57-4105-46aa-a109-f9448278ab0b",
   "metadata": {},
   "source": [
    "CLIP:\n",
    "- concepts\n",
    "- embedding text and images\n",
    "- measuring similarity\n",
    "- Use as a loss (imstack demo)\n",
    "- Need for transformations\n",
    "- export some functions for later\n",
    "- CLOOB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd961ce-2bbf-4b80-b757-97bb8e7eb214",
   "metadata": {},
   "source": [
    "![clip diagram](https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/CLIP.png)\n",
    "\n",
    "They train an image encoder and a text encoder simultaneously on a dataset of 400 million (image, text) pairs collected from the internet. Images and text are both transformed into encodings that can be directly compared.\n",
    "\n",
    "From the paper, \"To do this, CLIP learns a multi-modal embedding space by jointly training an image encoder and text encoder to maximize the cosine similarity of the image and text embeddings of the N real pairs in the batch while minimizing the cosine similarity of the embeddings of the N2 âˆ’ N incorrect pairings. We optimize a symmetric cross entropy loss over these similarity scores\"\n",
    "\n",
    "The OpenAI team actually trained several different image encoders. Some were based on the ResNet architecture, a fairly popular Convolutional Neural Network type. Others were based on something called a Vision Transformer. The text encoder is also a transformer model. We haven't covered transformer models yet - for now we just need to know that they're great at modelling sequence data like text, and it turns out that by representing an image as a sequence of smaller image patches they're pretty good at dealing with images too.\n",
    "\n",
    "Let's load up CLIP and encode some different inputs and then see what we can do with this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b97a0ad-db12-4582-bc81-5e41191bea73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO talk about different clip models, naming standard, open clip efforts etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

[
  {
    "objectID": "finetuning_pretrained_models.html",
    "href": "finetuning_pretrained_models.html",
    "title": "Fine-Tuning Pretrained Networks for Image Classification",
    "section": "",
    "text": "import torchvision\nfrom fastai.vision.all import *"
  },
  {
    "objectID": "finetuning_pretrained_models.html#replicating-some-of-this-in-pytorch",
    "href": "finetuning_pretrained_models.html#replicating-some-of-this-in-pytorch",
    "title": "Fine-Tuning Pretrained Networks for Image Classification",
    "section": "Replicating (some of) this in PyTorch",
    "text": "Replicating (some of) this in PyTorch\nWhat would this look like in raw PyTorch? Let’s clear out some GPU memory and then try to replicate the result above:\n\nimport gc\ndel learn\ngc.collect()\ntorch.cuda.empty_cache()\n\nThe first component is the dataloaders. In addition to loading the images, resizing and normalizing the data, reading in the labels and so on, the dataloaders also do something called ‘data augmentation’, where each image is randomly transformed using a user-specified list of possible transforms (we use the pre-defined aug_transforms above). This means that every time we get a batch of data we’ll see the model inputs (images that have been scaled and transformed with some augmentation) and the labels:\n\nxb, yb = dls.one_batch()\nxb.shape, yb\n\n(torch.Size([64, 3, 224, 224]),\n TensorCategory([15,  1, 23, 20,  7,  7, 26, 30, 27, 22, 27,  2, 15,  0,  2, 31,\n                 24, 10,  2,  4, 33, 22, 28,  0, 27, 16, 34, 19,  7, 16,  6, 18,\n                  2,  3, 27, 26, 30, 28, 13, 18, 28, 16, 31, 19, 21, 19, 14,  3,\n                 13, 15, 27,  2,  2, 24,  0, 27, 32, 13, 34, 19, 23,  5, 25, 10],\n                device='cuda:0'))\n\n\nIf you want to see the string versions of the labels, you can look them up in dls.vocab:\n\nlen(dls.vocab), dls.vocab\n\n(37,\n ['Abyssinian', 'Bengal', 'Birman', 'Bombay', 'British_Shorthair', 'Egyptian_Mau', 'Maine_Coon', 'Persian', 'Ragdoll', 'Russian_Blue', 'Siamese', 'Sphynx', 'american_bulldog', 'american_pit_bull_terrier', 'basset_hound', 'beagle', 'boxer', 'chihuahua', 'english_cocker_spaniel', 'english_setter', 'german_shorthaired', 'great_pyrenees', 'havanese', 'japanese_chin', 'keeshond', 'leonberger', 'miniature_pinscher', 'newfoundland', 'pomeranian', 'pug', 'saint_bernard', 'samoyed', 'scottish_terrier', 'shiba_inu', 'staffordshire_bull_terrier', 'wheaten_terrier', 'yorkshire_terrier'])\n\n\nWe’ll just use the dataloaders here rather than re-creating them ourselves. The more intersting bit is: how do we turn a model trained on 1000 imagenet classes into one capable of classifying 37 dog breeds?\nFirst, we load the pretrained model:\n\nmodel = resnet34(weights=torchvision.models.ResNet34_Weights.DEFAULT)\n\nThen we replace the final fully connected layer (model.fc, although the naming will vary depending on your choice of model/architecture) with our own new layer. This gives us a modified version of the model with 37 outputs:\n\nmodel.fc = nn.Linear(512, 37)\nmodel.to(xb.device)\nwith torch.no_grad():\n    preds = model(xb)\npreds.shape\n\ntorch.Size([64, 37])\n\n\n\n# model # Uncomment to view the full model\n\nWe can train this with cross entropy loss (see Lesson 3):\n\nloss_fn = CrossEntropyLossFlat()\nloss_fn(preds, yb)\n\nTensorBase(3.8790, device='cuda:0')\n\n\nImportantly, the final layer we added has been initialized with random parameters, so we should probably train those a bit first, without modifying the rest of the model too drastically. After all, it has already learnt lots of useful features and we wouldn’t want to mess it up while this random final layer was not making good use of them. To do this, we freeze all the rest of the model parameters (setting requires_grad to False) and train for one epoch:\n\n[p.shape for p in model.fc.parameters()]\n\n[torch.Size([37, 512]), torch.Size([37])]\n\n\n\nlosses = []\n\n# Freeze all layers (requires_grad=False)\nfor p in model.parameters():\n    p.requires_grad = False\n\n# Unfreeze final layer\nfor p in model.fc.parameters():\n    p.requires_grad= True\n\n# Optimizer on just these unfrozen parameters\nopt = torch.optim.Adam(model.fc.parameters(), lr=1e-3)\n\n# Train for one epoch\nfor xb, yb in dls.train:\n    preds = model(xb)\n    loss = loss_fn(preds, yb)\n    losses.append(loss.item())\n    loss.backward()\n    opt.step()\n    opt.zero_grad()\n\nPlotting the losses:\n\nplt.plot(losses)\n\n\n\n\nNow we can unfreeze the rest of the parameters and train a bit longer, using a lower learning rate:\n\n# Unfreeze the rest of the model parameters\nfor p in model.parameters():\n    p.requires_grad = True\n\n# New optimizer and train a bit more at a lower learning rate\nopt = torch.optim.Adam(model.parameters(), lr=1e-4)\nfor xb, yb in dls.train:\n    preds = model(xb)\n    loss = loss_fn(preds, yb)\n    losses.append(loss.item())\n    loss.backward()\n    opt.step()\n    opt.zero_grad()\n\nPlotting the losses, showing this second epoch in yellow:\nYou could train for multiple epochs, explore using some sort of learning rate schedule and otherwise tweak this training procedure to try to improve performance further. But before you do, let’s see how we might calculate accuracy on the validation set:\nNot bad! It is pretty convenient to have all of these features (automatic evaluation of metrics, data augmentation, learning rate schedules, tracking model freezing/unfreezing…) built into fastai, but they aren’t magic - we can totally do these ourselves too!\nTODO script doing this all from scratch in PyTorch\nTODO talk about how having logging is nicer than waiting for a training loop to finish before you can see stats\nTODO Add a bit more explanation"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Generative Landscape",
    "section": "",
    "text": "UPDATE: I’m taking a break from development for a while, my apologies. The first ~5 lessons are mostly done, I’ve removed the unfinished ones (red in the diag below) from the outline on the left for now.\nThe Discord is a good place to go for updates and to find people who might be keen on discussing things and working on projects.\n\n\n\n\n\nCheck out the Getting Started page for an overview of the course and more information on things like study groups.\nCheck out the Library page for information on the tglcourse library that accompanies the course.\n\nFAQs\nSome course-related questions that have tricked in:  - ‘Any prerequisites?’: If you’re comfortable with a bit of Python and using Jupyter Notebooks you should be ready to take this course. No prior deep learning knowledge is assumed, and although we will dive fairly deep fairly quickly I’ve tried to link lots of resources wherever possible.  - ‘How long is the course?’: There are 15 core lessons plus a number of bonus notebooks. You can take them at whatever pace you find enjoyable, or join in with a study group on Discord to work through one a week. - ‘Does the HuggingFace Diffusion Model Class supercede this?’: I’ve teamed up with HF to help build their diffusion model class, sharing a lot of material between it and this course. Both will have unique things to add - I’d recommend signing up for theirs even if you’re working through ‘The Generative Landscape’ as well, since there will be extra projects and fun community activities to get involved in with that one too.  - ‘Can genereative landscape be real?’: Yes, if you subscribe to David Chalmers ‘simulation realism’ ;) - ‘Will it include stable diffusion?’: Yes, see lesson 14 - ‘Will we learn how to adapt these method to 3d?’: At some point I’d love to add more 3D related content - stay tuned for bonus notebooks once the craziness of the course launch calms down. - ‘I hope this includes generating text!’: It does! Lessons 9 - 11 deal with modelling sequences, and should set you up with everything you need to make models which can spew out AI-generated gibberish all day long. - ‘My question isn’t in the FAQs?’: OK, so I made this one up. But if you have a burning question, send it to me and I’ll add it here.\nCI status: \nPage stats: Total Hits:  Page visitors:"
  },
  {
    "objectID": "interfaces_with_gradio.html",
    "href": "interfaces_with_gradio.html",
    "title": "Creating Quick Interfaces with Gradio",
    "section": "",
    "text": "In this notebook we’ll see how Gradio let’s you quickly turn a function into a beautiful-looking web application that you can share with others to demo your latest model or pipeline.\nI’ve kept this fairly simple, and tried to show the process I’d follow if I was making this from scratch rather than just the final result."
  },
  {
    "objectID": "interfaces_with_gradio.html#what-should-our-demo-do",
    "href": "interfaces_with_gradio.html#what-should-our-demo-do",
    "title": "Creating Quick Interfaces with Gradio",
    "section": "What Should Our Demo Do?",
    "text": "What Should Our Demo Do?\nAs a first step, we need to decide what our demo should do! Here I load an image and experiment with fitting a small Siren network to it as a way to create an interesting blurred effect:\n\nfrom tglcourse.generation_utils import MSELossToTarget, SirenGenerator, optimise\nfrom tglcourse.utils import *\n\nLoading the image:\n\nim = load_image_pil('images/frog.png').resize((256, 256))\nim\n\n\n\n\nCreating a SirenGenerator (see the ‘Fun with Generators and Losses’ bonus notebook):\n\ngen = SirenGenerator(dim_hidden=32, num_layers=3)\n\nAt the moment the output of the generator doesn’t look like much:\n\ntensor_to_pil(gen())\n\n\n\n\nCreate a loss function that compares an input to a target image, in this case our frog:\n\nloss = MSELossToTarget(pil_to_tensor(im), size=256)\nloss(gen())\n\ntensor(0.1525, grad_fn=<MeanBackward0>)\n\n\nOptimise the parameters of the generator based on this loss:\n\noptimise(gen, [loss], n_steps=30)\n\n\n\n\n\n\n\n\n\n\n\nAnd get the result back as a PIL image:\n\ntensor_to_pil(gen())\n\n\n\n\nOK, so far so good - let’s try to serve this process up in an easy-to-use interface."
  },
  {
    "objectID": "interfaces_with_gradio.html#wrapping-our-code-in-a-function",
    "href": "interfaces_with_gradio.html#wrapping-our-code-in-a-function",
    "title": "Creating Quick Interfaces with Gradio",
    "section": "Wrapping our code in a function",
    "text": "Wrapping our code in a function\nThe first step to turning this into a nice web app is wrapping the above in a nice function:\n\nfrom IPython.utils import io\n\n\ndef sirenify(image, size=256):\n    gen = SirenGenerator()\n    loss = MSELossToTarget(pil_to_tensor(image), size=int(size))\n    with io.capture_output() as captured: # Hide output to keep things clean - remove for debugging\n        optimise(gen, [loss], n_steps=30)\n    return tensor_to_pil(gen())\n\n\nsirenify(im)\n\n\n\n\nYou can see it takes several inputs, each of which we’ll be able to set using the gradio interface. In this case we want to return an image, but you can return multiple different outputs (maybe an image, a caption and a confidence score)."
  },
  {
    "objectID": "interfaces_with_gradio.html#making-the-gradio-interface",
    "href": "interfaces_with_gradio.html#making-the-gradio-interface",
    "title": "Creating Quick Interfaces with Gradio",
    "section": "Making the Gradio Interface",
    "text": "Making the Gradio Interface\nWith the defined, we can create the interface like so:\n\nimport gradio as gr\n\n\niface = gr.Interface(fn=sirenify, inputs=[gr.Image(type='pil'), gr.Number(value=256)], outputs=gr.Image())\niface.launch(share=True)\n\nPretty simple! You should be able to upload an image and click ‘Submit’, and if all goes well you’ll soon see the result in the output tab on the right. For example:"
  },
  {
    "objectID": "interfaces_with_gradio.html#sharing-with-the-world",
    "href": "interfaces_with_gradio.html#sharing-with-the-world",
    "title": "Creating Quick Interfaces with Gradio",
    "section": "Sharing with the World",
    "text": "Sharing with the World\nIn the example above we used share=True and got a public URL. You can send this to someone for a quick demo, but the link will expire in 72 hours and the demo is running on your machine. If you want to share your app somewhere more permanent, the easiest option is HuggingFace Spaces.\nGetting an app running on there is as simple as packaging up your code into a script and adding requirements to a ‘requirements.txt’ file. The Gradio docs include some examples and there are plenty of tutorials out there if you get stuck. However, my favourite resource for seeing how to do this is reading the source code for existing spaces! For any space on the HuggingFace Hub! For example, check out the stable diffusion space, click on ‘Files and Versions’ and open app.py to see how they handle a more complex layout with custom styling and a bunch of integrated examples.\nThis excellent guide shows how you can take an experiment such as our demo above and export the relevant code straight from a notebook into an app.py file using NBDev. Yet another place where NBDev just seems like magic; I think this is how I will build all my spaces going forward!"
  },
  {
    "objectID": "bonus_material_intro.html",
    "href": "bonus_material_intro.html",
    "title": "Bonus Material",
    "section": "",
    "text": "This section contains all the useful bits that didn’t quite fit in the core course curriculum. It’s a little sparse right now, but post launch there are a host of extra topics planned :)"
  },
  {
    "objectID": "gan2.html",
    "href": "gan2.html",
    "title": "Lesson 8: Conditional GANs, Training Tricks and GANs+CLIP",
    "section": "",
    "text": "Introduce the idea of conditioning\nPlay with BigGAN\nMaybe key ideas of stylegan…\nSHow GauGAN as amazing conditioning\nOptimize a latent with CLIP\nPage stats: Total Hits:  Page visitors:"
  },
  {
    "objectID": "building_nns.html",
    "href": "building_nns.html",
    "title": "Lesson 3: Neural Networks and Loss Functions",
    "section": "",
    "text": "In Lesson 2 we said that if we have (1) a function defined by some parameters and (2) a loss function or some measure of how well it performs then we can optimize the parameters using something like gradient descent to try and improve performance.\nWhat does that ‘function’ look like? In many cases, the answer is some sort of neural network. Artificial Neural Networks (ANNs) have been around for many years [citation needed] but it is only in the psat decade or so that we’ve really figured out ways to train them well. The reason they’re so great is that they are pretty good general function approximators. In this notebook we’ll start with a very simple network approximating a fairly simple function, and then we’ll move on to solving a classic ML problem: classifying hand-written digits.\nIn the bonus notebook [TODO finish, link] and tomorrows lesson [TODO finish, link] we will build on these foundations to see how we can use modernt pre-trained networks to solve much more complex problems than those presented here.\nFor this notebook the code examples move fairly quickly - the lesson run-through will try to break them down more so check that out if you’re having difficulty."
  },
  {
    "objectID": "building_nns.html#a-simple-neural-network",
    "href": "building_nns.html#a-simple-neural-network",
    "title": "Lesson 3: Neural Networks and Loss Functions",
    "section": "A Simple Neural Network",
    "text": "A Simple Neural Network\n\n\n\n\n\nLet’s start by making a smallish network to solve a small (contrived) problem. We’ll generate some data, and as in the previous lesson we’d like our network to learn the relationship between our input data (x) and output data (y).\n\n# Generating some data (inputs and targets)\nn_samples = 64\ninputs = torch.linspace(-1.0, 1.0, n_samples).reshape(n_samples, 1)\nnoise = torch.randn(n_samples, 1) / 5\ntargets = torch.sin(3.14 * inputs) + noise\nplt.figure(figsize=(8, 5))\nplt.scatter(inputs, targets, c='c')\nplt.xlabel('x (inputs)')\nplt.ylabel('y (targets)')\nplt.show()\n\n\n\n\nHere’s how we can make a neural network, leaning on PyTorch’s handy functions:\n\n## A Wide neural network with a single hidden layer\nclass WideNet(nn.Module):\n\n    def __init__(self, n_cells=512): # Initialize our network\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(1, n_cells), # One input, n_cells outputs\n            nn.Tanh(), # Our non-linearity - there are many on offer!\n            nn.Linear(n_cells, 1), # n_cells inputs, one output\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n\nWe’re inheriting from the nn.Module class, which gives us some bonus features. For example, instead of directly calling the forward method (which passes the data through the different layers) we can just create our network and call it like a function:\n\nwn = WideNet()\nprint('Input shape:', inputs.shape)\nout = wn(inputs) # This passes our data in as the input to the forward method defined above\nprint('Output shape:', out.shape)\nprint('PyTorch sumary of wn:')\nwn\n\nInput shape: torch.Size([64, 1])\nOutput shape: torch.Size([64, 1])\nPyTorch sumary of wn:\n\n\nWideNet(\n  (layers): Sequential(\n    (0): Linear(in_features=1, out_features=512, bias=True)\n    (1): Tanh()\n    (2): Linear(in_features=512, out_features=1, bias=True)\n  )\n)\n\n\nThis network includes some layers that have learnable parameters. We can access all of these via wn.parameters() - in this case we get four sets of parameters - the weights and biases for each of the two linear layers. Feel free to experiment with the network definition and see how this changes:\n\n[p.shape for p in wn.parameters()]\n\n[torch.Size([512, 1]),\n torch.Size([512]),\n torch.Size([1, 512]),\n torch.Size([1])]\n\n\nTime for our training loop - compare this with the optimization loop we in the previous lesson (spoiler: they’re the same!). We’re optimizing the parameters of our neural network - all the weights and biases in the different layers.\n\n# Create our network\nwide_net = WideNet()\n\n# Create a mse loss function\nloss_function = nn.MSELoss()\n\n# Stochstic Gradient Descent optimizer\noptimizer = torch.optim.Adam(wide_net.parameters(), lr=1e-3)\n\n# The training loop\nlosses = []  # keeping recods of loss\nfor i in range(500): # 500 'epochs' of training\n    optimizer.zero_grad()  # set gradients to 0\n    predictions = wide_net(inputs)  # Compute model prediction (output)\n    loss = loss_function(predictions, targets)  # Compute the loss\n    loss.backward()  # Compute gradients (backward pass)\n    optimizer.step()  # update parameters (optimizer takes a step)\n\n    # Storing our loss for later viewing\n    losses.append(loss.item())\n\n# Plot the losses over time\nplt.plot(losses) # Plot the losses over time\n\n\n\n\nTHINK/DISCUSS: Go line-by-line through the code above - does it make sense?\nNotice we don’t have to set requires_grad manually on any of the parameters, since the learnable parameters in each layer are grouped automatically by wide_net.parameters() (inspect that and see what it contains).\n\nplt.scatter(inputs, targets)\nplt.plot(inputs, wide_net(inputs).detach(), c='red', label='Predictions')\n# Create a new, untrained widenet and plot those predictions for comparison\nnew_wn = WideNet()\nplt.plot(inputs, new_wn(inputs).detach(), c='yellow', label='Predictions (untrained network)')\nplt.legend()\nplt.show()\n\n\n\n\n\n# Exercise: Create a neural network with two hidden layers and train it on this same task..."
  },
  {
    "objectID": "building_nns.html#a-lightning-overview-of-convnets",
    "href": "building_nns.html#a-lightning-overview-of-convnets",
    "title": "Lesson 3: Neural Networks and Loss Functions",
    "section": "A Lightning Overview of ConvNets",
    "text": "A Lightning Overview of ConvNets\nSo-called ‘dense’ networks are useful in some cases, but we need to be mindful of the number of parameters required to solve certain types of problems. For example, consider the case of image recognition - our inputs consist of thousands of individual pixel values. A dense network that could take in 500px images and then run them through many hidden layers ends up with millions or billions of parameters, which can make training tricky.\nIn addition, each pixel feeds into a different part of the network. When we look at how the vision system works in the brain, or just think about what we’d want in a computer vision system, we’ll start to hit requirements that might not be easy to satisfy with a simple MLP network. Fortunately, we have some tricks to imporve things! Here’s another video that takes us through a key idea in deep learning for images: CNNs\nHere’s another video from the (free and I CC-licenced) neuromatch course that gives a little more background:\n\nhtml = ipd.display(ipd.IFrame(src=\"https://www.youtube.com/embed/AXO-iflKa58\", width=\"560\", height=\"315\"))\nhtml\n\n\n        \n        \n\n\nThe following interactive website is a great way to get an intuition for both how convolution works. You can see each learned filter and the corresponding output. It also shows a second key idea: pooling. By ‘downsampling’ the outputs of successinve convolution layers we end up with fewer and fewer activations, each representing more and more of the input image. Play around a bit until you’re sort of happy with the basic concepts (see the video for more discussion and explanations) and then move on to the next section, where we’ll build our own very similar network.\n\n# html = ipd.display(ipd.HTML('<iframe width=\"1200\" height=\"600\" src=\"https://adamharley.com/nn_vis/cnn/3d.html\" title=\"CNN Visualization\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>'))\n# html\n\nFor more on what convolution is, how the different hyperparameters (padding, stride etc) do and a general overview of CNNs, see https://poloclub.github.io/cnn-explainer/"
  },
  {
    "objectID": "building_nns.html#the-dataset",
    "href": "building_nns.html#the-dataset",
    "title": "Lesson 3: Neural Networks and Loss Functions",
    "section": "The Dataset",
    "text": "The Dataset\nWe’ll use the classic MNIST dataset (as shown in the video and examples above). But these same ideas apply to more complex image recognition\n\n#@title Loading the data\nmnist_dl_train = get_mnist_dl(batch_size=128, split='train')\nmnist_dl_test = get_mnist_dl(batch_size=128, split='test')\n\n# Get one batch of data\nbatch = next(iter(mnist_dl_train))\n\ndata_shape = (1, 28, 28)\n\n# Plot a few examples\nfig, axs = plt.subplots(1, 4, figsize=(12, 4))\nfor i in range(4):\n    im, label = batch['image'][i], batch['label'][i]\n    axs[i].imshow(im.squeeze(), cmap='gray')\n    axs[i].set_title(label)\n\nReusing dataset mnist (/root/.cache/huggingface/datasets/mnist/mnist/1.0.0/fda16c03c4ecfb13f165ba7e29cf38129ce035011519968cdaf74894ce91c9d4)\nReusing dataset mnist (/root/.cache/huggingface/datasets/mnist/mnist/1.0.0/fda16c03c4ecfb13f165ba7e29cf38129ce035011519968cdaf74894ce91c9d4)"
  },
  {
    "objectID": "building_nns.html#defining-our-network",
    "href": "building_nns.html#defining-our-network",
    "title": "Lesson 3: Neural Networks and Loss Functions",
    "section": "Defining Our Network",
    "text": "Defining Our Network\nThe convolution operation is handled by the nn.Conv2d layer. Uncomment the next line to view some info about this:\n\n# ?nn.Conv2d\n\nLet’s use nn.Conv2D to convolve this image with some random kernels:\n\nconv_test = nn.Conv2d(in_channels=1, out_channels=12, kernel_size=5, padding=0)\nimage = batch['image'][0].unsqueeze(0) # unsqueeze the first image to make this a 'batch of size 1'\nprint('Input shape: ', image.shape) # One channel greyscale image\nprint('Output shape: ', conv_test(image).shape) # 12 output channels (from 12 kernels)\n\nInput shape:  torch.Size([1, 1, 28, 28])\nOutput shape:  torch.Size([1, 12, 24, 24])\n\n\nNote the initial shape is slightly smaller - how does padding change this? Does the output shape make sense?\nThis layer has some trainable parameters: the kernels. Let’s check these out:\n\nconv_test.weight.shape # 12 filters, each 1x5x5\n\ntorch.Size([12, 1, 5, 5])\n\n\nTHINK: Does the number of parameters in this layer depend on the input image size?\nHere’s a network that uses these layers (along with nn.MaxPool2d for the downsampling/pooling). We could use nn.Sequential as in the previous example, but I’d like to show another common style here. We define all the layers we’ll need in init but only in the forward() method do we actually specify how that data should flow through the network.\n\n# Network definition\nclass MiniCNN(nn.Module):\n    def __init__(self):\n        super(MiniCNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3)\n        self.conv2 = nn.Conv2d(32, 64, 3)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10) # 10 outputs (for 10 digits)\n        self.pool = nn.MaxPool2d(2)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = self.pool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        return x"
  },
  {
    "objectID": "building_nns.html#the-training-loop",
    "href": "building_nns.html#the-training-loop",
    "title": "Lesson 3: Neural Networks and Loss Functions",
    "section": "The Training Loop",
    "text": "The Training Loop\n\n\n\ntraining\n\n\nHere’s a training loop that should now be getting quite familiar. A few noteworthy things: - We can’t push all the images through in one go, so within each epoch (i.e. each full psas through the data) we do multiple batches. This is when ‘Gradient Descent’ becomes ‘Stochastic Gradient Descent’ or ‘Mini-batch GD’ depending on who you’re talking to. PyTorch does the batching for us via something called a DataLoader. - We’d like to train on the GPU, so we need to make sure both the model and the data are on the right device with .to(device) (device is defined earlier). - We’re using a loss function that is good for classification tasks: nn.CrossEntropyLoss(). Accuracy has ‘steps’ and so it makes differentiation tricky. By treating the outputs of the network as probabilities we can see how confident it is that something is in a specific class while keeping everything continuous and differentiable. Don’t worry too much about this :)\n\n# Set up model, loss and optimizer\nmodel = MiniCNN().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\nlosses = []\n\n# The training loop\nfor batch in tqdm(mnist_dl_train, unit='batch'):\n    data, target = batch['image'], batch['label']\n    data, target = data.to(device), target.to(device)\n    optimizer.zero_grad()\n    output = model(data)\n    loss = criterion(output, target)\n    loss.backward()\n    optimizer.step()\n\n    # Log the loss\n    losses.append(loss.item()) # .item makes a copy of just the value, detached from any gradient calculations.\n\nplt.plot(losses)\n\n\n\n\n\n\n\nASIDE: If you’re on a machine with a GPU, remove all to(device) in the above code - is it slower on CPU?"
  },
  {
    "objectID": "building_nns.html#evaluation---how-well-does-it-do",
    "href": "building_nns.html#evaluation---how-well-does-it-do",
    "title": "Lesson 3: Neural Networks and Loss Functions",
    "section": "Evaluation - how well does it do?",
    "text": "Evaluation - how well does it do?\n\n\n\ntesting\n\n\n\n## Testing\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n  # Iterate through test set minibatchs\n  for batch in tqdm(mnist_dl_test):\n    data, labels = batch.values() # TODO fix this in data eg\n    data, labels = data.to(device), labels.to(device) # Move the data to GPU for faster execution.\n    y = model(data) # Forward pass\n    predictions = torch.argmax(y, dim=1) # The model has ten outputs, one for each digit. Here we take the index with the highest output\n    correct += torch.sum((predictions == labels).float())\n    total += labels.shape[0]\n\nprint(f'Test accuracy: {correct/total * 100:.2f}%')\n\n\n\n\nTest accuracy: 88.37%\n\n\n\n# Exercise: See how good you can get! Tweak the architecture, the hyperparameters, the training time, the optimizer... go wild ;)\n# You may want to try doing multiple passes through the dataloader - aka multiple epochs.\n\nPhew! Welcome to deep learning :)\nWe’re learning just enough to move on with the course, but these are some big topics and we’re barely scratching the surface. If you’re interested in more of the theory or implementing some of these ideas from scratch, you might like to check out the content at https://deeplearning.neuromatch.io/ or one of the many deep learning courses on various MOOC platforms. If you’d like a more top-down approach to doing practical deep learning realy well, I can’t recommend the fastai course highly enough.\nThere is also a great from-scratch lesson from Andrej Karpathy going into all the nitty-gritty details of how gradients are calculated and so on: https://www.youtube.com/watch?v=VMj-3S1tku0"
  },
  {
    "objectID": "building_nns.html#loss-functions",
    "href": "building_nns.html#loss-functions",
    "title": "Lesson 3: Neural Networks and Loss Functions",
    "section": "Loss Functions",
    "text": "Loss Functions\nIt’s worth talking just a little more here about the concept of loss functions.\nIn our first task, we wanted to predict a continuous output, y. How do we compare the network outputs with the known values in such a way as to force them to be as close as possible? A popular choice is the mean squared error (MSE) or the root mean squared error (RMSE). We used the builtin PyTorch method but we can also implement this ourselves and verify that the result is the same on some dummy data:\n\ntargets = torch.tensor([0.2, 0.7, 0.1])\npredictions = torch.tensor([0.25, 0.6, 0.3])\nprint('Result using nn.MSELoss()(predictions, targets):', nn.MSELoss()(predictions, targets)) # Think: does order matter?\nprint('Result using ((targets - predictions)**2).mean():', ((targets - predictions)**2).mean())\n\nResult using nn.MSELoss()(predictions, targets): tensor(0.0175)\nResult using ((targets - predictions)**2).mean(): tensor(0.0175)\n\n\nMSE loss is sometimes called L2 loss. You could also try the Mean Absolute Error (MAE), aka L1 loss. The choice comes down to what we’d like to penalize. Because the error is squared in MSE, larger errors result in a much larger loss, while small errors incur only a small loss. This is preferable in many cases - we’d prefer to explicitly avoid massive errors!\n\nClassification\nHow would we make a loss function for classification? You could try something like accuracy: for a given batch, loss = number_wrong/number_of_examples, for example. But there’s a problem. Our updates require gradients, and accuracy will not be smooth and differentiable! So, we need to come up with a better loss function.\nOne candidate is MSE! We can encode our labels using one-hot encoding to get 10 values, 0 everywhere except for the column corresponding to the right class label:\n\n# Exercise: Can you use MSE loss for classification?\n\n# You can encode some class labels like so:\nlabels = torch.tensor([0, 1, 7, 3, 9])\none_hot_encoded = F.one_hot(labels, num_classes=10)\none_hot_encoded\n\ntensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])\n\n\nSet up a network with 10 outputs, and use MSE to force it towards predicting low outputs for the wrong labels and high labels for the right ones. This should work, and was how the first digit recognition network was trained!\nIn practice, this is not the way we usually do it. Cross Entropy Loss takes the model outputs and does an operation called SoftMax on them (which scales the outputs to sum to 1) so that they can be interpreted as probabilities. Then it calculates a score based on the log of the probability associated with the correct label. Technically, CE is comparing two distributions (the predicted probability distribution from the model, and the known label distribution) and there’s lots of information theory and confusing terminology you are welcome to read up on. But the core of the matter is this: for labels very close to the true values, the loss will be small. For labels that are far off, the loss is high. And thanks to the maths, we can interpret the softmax of the model outputs as probabilities showing how ‘confident’ the model is.\nHere is an attempt at re-creating CrossEntropyLoss in code:\n\nlabels = torch.tensor([0, 2, 1, 0])\npredictions = torch.tensor([[1.95, -0.6, -0.3],\n                            [-0.25, -0.6, 2.3],\n                            [0.05, 3.6, -0.3],\n                            [3.7, -1.1, -1.2]])\nprint('Result using nn.CrossEntropyLoss()(labels, predictions):', nn.CrossEntropyLoss()(predictions, labels))\n\n# Softmax of raw preds (usually model outputs)\nprobs = nn.Softmax(dim=1)(predictions)\nprint('Softmax output:\\n', probs)\n\n# Get the probabilities corresponding to the correct labels\nrelevant_probs = probs[range(labels.shape[0]), labels]\nprint('Relevant Probabilities:', relevant_probs)\n\n# Calculate \nneg_log = -relevant_probs.log()\nprint('Negative Liklihoods:', neg_log)\nprint('Mean NL on softmax outputs:', neg_log.mean())\n\nResult using nn.CrossEntropyLoss()(labels, predictions): tensor(0.0892)\nSoftmax output:\n tensor([[0.8450, 0.0660, 0.0891],\n        [0.0689, 0.0486, 0.8825],\n        [0.0274, 0.9533, 0.0193],\n        [0.9846, 0.0081, 0.0073]])\nRelevant Probabilities: tensor([0.8450, 0.8825, 0.9533, 0.9846])\nNegative Liklihoods: tensor([0.1685, 0.1250, 0.0478, 0.0156])\nMean NL on softmax outputs: tensor(0.0892)\n\n\n\n# Exercise try a version of widenet on mnist as well - how good can you get it?"
  },
  {
    "objectID": "building_nns.html#pretrained-networks",
    "href": "building_nns.html#pretrained-networks",
    "title": "Lesson 3: Neural Networks and Loss Functions",
    "section": "Pretrained Networks",
    "text": "Pretrained Networks\nTraining a network from scratch can be time-consuming and require LOTs of data…\nPage stats: Total Hits:  Page visitors:"
  },
  {
    "objectID": "ethics.html",
    "href": "ethics.html",
    "title": "Ethics in Generative Modelling",
    "section": "",
    "text": "Hopefully find some people for good discussions here :)"
  },
  {
    "objectID": "sequence_1.html",
    "href": "sequence_1.html",
    "title": "Lesson 9: Introduction to Sequence Modelling",
    "section": "",
    "text": "Intro - lots of things come as sequences, and text is common!"
  },
  {
    "objectID": "sequence_1.html#embeddings-working-with-tokens",
    "href": "sequence_1.html#embeddings-working-with-tokens",
    "title": "Lesson 9: Introduction to Sequence Modelling",
    "section": "Embeddings: Working With Tokens",
    "text": "Embeddings: Working With Tokens\nConcept of embeddings\nReference https://deeplearning.neuromatch.io/tutorials/W2D5_TimeSeriesAndNaturalLanguageProcessing/student/W2D5_Tutorial1.html and maybe link Lyle’s ‘Embeddings Rule’ video?\n\nimport torch\nfrom torch import nn\nfrom datasets import load_dataset\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import BPE\nfrom tokenizers.trainers import BpeTrainer\nfrom transformers import PreTrainedTokenizerFast"
  },
  {
    "objectID": "sequence_1.html#modelling-sequences-language-models",
    "href": "sequence_1.html#modelling-sequences-language-models",
    "title": "Lesson 9: Introduction to Sequence Modelling",
    "section": "Modelling Sequences: Language Models",
    "text": "Modelling Sequences: Language Models\nExplain the objective"
  },
  {
    "objectID": "sequence_1.html#tokenizing-text",
    "href": "sequence_1.html#tokenizing-text",
    "title": "Lesson 9: Introduction to Sequence Modelling",
    "section": "Tokenizing Text",
    "text": "Tokenizing Text\nHow do we split up text into tokens? We often talk about ‘words’ being the unit of text, but if we just go with a token for each word that we might encounter you’ll end up with a massive (1M) vocabulary filled with mostly obscure/misspelled words. But on the other hand letters would mean using far more tokens to represent the same sentence.\nOne solution.. explain wordpiece and co\nTokenizers: https://huggingface.co/docs/tokenizers/index\n\ntokenizer = Tokenizer.from_pretrained(\"bert-base-uncased\")\n\n\nencoding = tokenizer.encode('What a nice flooble!')\nprint('Encoding:', encoding)\n\nids = encoding.ids\nprint('ids:', ids)\n\nEncoding: Encoding(num_tokens=9, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\nids: [101, 2054, 1037, 3835, 13109, 9541, 3468, 999, 102]\n\n\n\nfor t in ids:\n    print(f'{t}:{tokenizer.decode([t])}')\n\n101:\n2054:what\n1037:a\n3835:nice\n13109:fl\n9541:##oo\n3468:##ble\n999:!\n102:\n\n\nWe have special tokens for start (101), end (102), symbols like ‘!’ (999) and separate tokens for a string like ‘oo’ or ‘ble’ if they don’t occur at the start of a word. Common words get a token, uncommon ones like flooble are broken down into components. THe full vocabulary size of this tokenizer is about 30,000 tokens:\n\nlen(tokenizer.get_vocab().items()) # The vocab size of this tokenizer\n\n30522"
  },
  {
    "objectID": "sequence_1.html#creating-our-own-tokenizer",
    "href": "sequence_1.html#creating-our-own-tokenizer",
    "title": "Lesson 9: Introduction to Sequence Modelling",
    "section": "Creating Our Own Tokenizer",
    "text": "Creating Our Own Tokenizer\nExplain\n\ndataset = load_dataset('tglcourse/abc_tunes', split='train').shuffle()\ndataset[0]\n\nUsing custom data configuration tglcourse--abc_tunes-5a89386c12e016f6\nReusing dataset parquet (/root/.cache/huggingface/datasets/tglcourse___parquet/tglcourse--abc_tunes-5a89386c12e016f6/0.0.0/7328ef7ee03eaf3f86ae40594d46a1cec86161704e02dd19f232d81eee72ade8)\n\n\n{'Title': \"Pat Hogan's One\",\n 'Time Signature': '2/4',\n 'L': '1/8',\n 'Key': 'Edor',\n 'Tune': 'D>D FA|dc BA|BE EF|GA/G/ FE|D>D FA|dcBA|Be Bc|d2 d2:||:eB eB|eB B>c|dA dA|dA A2|eB eB|eB B>c|dB AF|E2 E2:|'}\n\n\n\nvocab_size = 100 # Explore different vocab sizes\ntokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\ntrainer =BpeTrainer(special_tokens=[\"[UNK]\", \"[PAD]\"],\n                    vocab_size=vocab_size) \ntokenizer.train_from_iterator(dataset['Tune'], trainer)\n\n\n\n\n\n\n\ntokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer,\n                                    pad_token=\"[PAD]\",\n                                    unk_token=\"[UNK]\")\n\n\n# TODOSee the docs for pre-tokenizer options: https://huggingface.co/docs/tokenizers/quicktour\n\n\n# ??tokenizer.train\n\nPrinting out the decoded string (using ‘#’ as a separator) we ca see how the tune has turned into discrete tokens, some of thich are individual notes but many of which are combined.\n\nids = tokenizer.encode(dataset[0]['Tune']) # .ids if using the Tokenizer version\nprint(ids)\ndecoded = '#'.join([tokenizer.decode([t]) for t in ids])\nprint(decoded)\n\n[40, 34, 40, 4, 42, 37, 95, 71, 70, 4, 38, 37, 95, 38, 41, 4, 41, 42, 95, 43, 37, 19, 43, 19, 4, 42, 41, 95, 40, 34, 40, 4, 42, 37, 95, 71, 70, 38, 37, 95, 38, 72, 4, 38, 70, 95, 71, 22, 4, 71, 22, 30, 95, 95, 30, 72, 38, 4, 72, 38, 95, 72, 38, 4, 38, 34, 70, 95, 71, 37, 4, 71, 37, 95, 71, 37, 4, 37, 22, 95, 72, 38, 4, 72, 38, 95, 72, 38, 4, 38, 34, 70, 95, 71, 38, 4, 37, 42, 95, 41, 22, 4, 41, 22, 30, 95]\nD#>#D# #F#A#|#d#c# #B#A#|#B#E# #E#F#|#G#A#/#G#/# #F#E#|#D#>#D# #F#A#|#d#c#B#A#|#B#e# #B#c#|#d#2# #d#2#:#|#|#:#e#B# #e#B#|#e#B# #B#>#c#|#d#A# #d#A#|#d#A# #A#2#|#e#B# #e#B#|#e#B# #B#>#c#|#d#B# #A#F#|#E#2# #E#2#:#|\n\n\n\nprint(tokenizer.get_vocab())\n\n{'q': 84, '}': 96, 'í': 107, 'e': 72, '@': 36, 't': 87, 'ó': 109, 'U': 57, 'i': 76, '6': 26, '(': 12, 'b': 69, 'º': 101, '~': 97, 'f': 73, 'v': 89, '\\t': 3, 'Q': 53, 'S': 55, '0': 20, '#': 7, '4': 24, 'Z': 62, 'ä': 104, 'T': 56, 'D': 40, '\\x08': 2, '/': 19, '9': 29, 'G': 43, 'Y': 61, ')': 13, 'A': 37, '.': 18, 'I': 45, 'J': 46, 'á': 103, 'è': 105, 'V': 58, \"'\": 11, '^': 66, 'o': 82, '¬': 100, 'm': 80, '=': 33, 'p': 83, '!': 5, '[PAD]': 1, 'd': 71, 'z': 93, 'Ú': 102, '5': 25, ':': 30, '\"': 6, '+': 15, 'M': 49, '$': 8, '<': 32, '&': 10, 'H': 44, 'N': 50, 'h': 75, 'é': 106, 'ñ': 108, 'F': 42, '*': 14, '\\\\': 64, 'l': 79, '[UNK]': 0, '2': 22, 'L': 48, 'E': 41, 'K': 47, 'a': 68, '\\xa0': 98, 'R': 54, '7': 27, 'k': 78, '3': 23, ',': 16, '[': 63, '%': 9, 'ú': 110, 'n': 81, 'w': 90, 's': 86, 'c': 70, 'P': 52, '_': 67, 'y': 92, 'X': 60, ']': 65, 'r': 85, ';': 31, 'W': 59, '-': 17, 'u': 88, '8': 28, 'x': 91, 'g': 74, '{': 94, '|': 95, 'ª': 99, '>': 34, ' ': 4, '?': 35, '1': 21, 'B': 38, 'C': 39, 'O': 51, 'j': 77}"
  },
  {
    "objectID": "sequence_1.html#an-embedding-layer",
    "href": "sequence_1.html#an-embedding-layer",
    "title": "Lesson 9: Introduction to Sequence Modelling",
    "section": "An Embedding Layer",
    "text": "An Embedding Layer\n\nemb_dim = 32\nemb_layer = nn.Embedding(vocab_size, emb_dim)\nemb_layer\n\nEmbedding(100, 32)\n\n\n\nemb_layer(torch.tensor(ids)).shape # Passing our tokens through\n\ntorch.Size([106, 32])"
  },
  {
    "objectID": "sequence_1.html#a-simple-mlp",
    "href": "sequence_1.html#a-simple-mlp",
    "title": "Lesson 9: Introduction to Sequence Modelling",
    "section": "A simple MLP",
    "text": "A simple MLP\nSimilar to Karpathy’s makemore demo (TODO link)\nHow do we work with sequences of different lengths? Padding + truncation seem non-ideal…\n\nbatch_size=32\nseq_len=64\nbatch_ids = torch.randint(vocab_size, (batch_size,seq_len))\nbatch_ids.shape\n\ntorch.Size([32, 64])\n\n\n\nemb_layer(batch_ids).shape\n\ntorch.Size([32, 64, 32])\n\n\n\n# A minimal model (output sizes shown\nmodel = nn.Sequential(\n    nn.Embedding(vocab_size, emb_dim), # (batch_size, seq_length, emb_dim)\n    nn.Flatten(), # (batch_size, seq_length*emb_dim)\n    nn.Linear(emb_dim*seq_len, 64), # (batch_size, 64)\n    nn.ReLU(), # (batch_size, 64)\n    nn.Linear(64, 2), # (batch_size, 2)\n    \n)\nmodel(batch_ids).shape\n\ntorch.Size([32, 2])\n\n\nQ: What happens when word position changes? Q: Would this work on different length sequences? Q: think of more Qs"
  },
  {
    "objectID": "sequence_1.html#recurrent-neural-networks-and-lstms",
    "href": "sequence_1.html#recurrent-neural-networks-and-lstms",
    "title": "Lesson 9: Introduction to Sequence Modelling",
    "section": "Recurrent Neural Networks and LSTMs",
    "text": "Recurrent Neural Networks and LSTMs\nExplain the basic architecture\nhttps://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n\n\n\nan unrolled RNN\n\n\nMaybe demo?\nBrief hand-wave explanation of LSTMs and link ULMFIT and co for the curious\nGreat blog by colah https://colah.github.io/posts/2015-08-Understanding-LSTMs/ Karpathy on RNN effectiveness: http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n\n# Create the RNN\ninput_size = 10 # Number of features in the input (embedding dim)\nhidden_size = 20 # Number of features in the hidden state h\nnum_layers = 1 # Set to 2 for a 'stacked' RNN with 2 layers\nrnn = nn.RNN(input_size, hidden_size, num_layers) # The model\n\n# Run some dummy data through\n# Create the model with batch_first=True if you'd like the batch dimension to come first\nbatch_size = 8\ninput_length = 5\nx = torch.randn(5, batch_size, input_size)\nh0 = torch.randn(num_layers, batch_size, hidden_size)\noutput, hn = rnn(x, h0)\n\n# Check the output shapes\noutput.shape, hn.shape\n\n(torch.Size([5, 8, 20]), torch.Size([1, 8, 20]))\n\n\n\nclass MyRNNClassifier(nn.Module):\n    def __init__(self, input_size=10, hidden_size=20, num_layers=2):\n        super().__init__()\n        self.emb_layer = nn.Embedding(vocab_size, input_size)\n        self.rnn = nn.RNN(input_size, hidden_size, num_layers=num_layers)\n        self.mlp = nn.Linear(hidden_size, 2)\n        \n    def forward(self, x):\n        x = self.emb_layer(x) # TO embeddings (batch_size, seq_len, input_size)\n        net_output, h = self.rnn(x) # Through RNN (batch_size, seq_len, hidden_size)\n        averaged_output = net_output.mean(dim=1) # Take the mean of the outputs ('mean pooling')\n        result = self.mlp(averaged_output) # THrough the linear layer or MLP to get 2 outputs (assuming binary classification)\n        return result\n\n\nnet = MyRNNClassifier()\nnet(batch_ids).shape\n\ntorch.Size([32, 2])\n\n\n\nsum([p.numel() for p in net.parameters()])\n\n2522\n\n\n\n[p.shape for p in net.parameters()]\n\n[torch.Size([100, 10]),\n torch.Size([20, 10]),\n torch.Size([20, 20]),\n torch.Size([20]),\n torch.Size([20]),\n torch.Size([20, 20]),\n torch.Size([20, 20]),\n torch.Size([20]),\n torch.Size([20]),\n torch.Size([2, 20]),\n torch.Size([2])]\n\n\n\n# TODO try on some data\n\nLSTMs\nhttps://colah.github.io/posts/2015-08-Understanding-LSTMs/\nsomething something memory\n\nclass MyLSTMClassifier(nn.Module):\n    def __init__(self, input_size=10, hidden_size=20, num_layers=2):\n        super().__init__()\n        self.emb_layer = nn.Embedding(vocab_size, input_size)\n        self.rnn = nn.LSTM(input_size, hidden_size, num_layers=num_layers)\n        self.mlp = nn.Linear(hidden_size, 2)\n        \n    def forward(self, x):\n        x = self.emb_layer(x) # TO embeddings (batch_size, seq_len, input_size)\n        net_output, h = self.rnn(x) # Through RNN (batch_size, seq_len, hidden_size)\n        averaged_output = net_output.mean(dim=1) # Take the mean of the outputs ('mean pooling')\n        result = self.mlp(averaged_output) # THrough the linear layer or MLP to get 2 outputs (assuming binary classification)\n        return result\n\n\nnet = MyLSTMClassifier()\nnet(batch_ids).shape\n\ntorch.Size([32, 2])\n\n\n\nsum([p.numel() for p in net.parameters()])\n\n6962"
  },
  {
    "objectID": "sequence_1.html#using-learned-representations",
    "href": "sequence_1.html#using-learned-representations",
    "title": "Lesson 9: Introduction to Sequence Modelling",
    "section": "Using Learned Representations",
    "text": "Using Learned Representations\nDo review classification or something using a learned embedding combined with an RNN? Or model tunes and then classify into type/key/mode?\nYeah tunes will be good. LM objective first, then re-training\n\nclass LM(nn.Module):\n    def __init__(self, input_size=32, hidden_size=128, num_layers=2, vocab_size=200):\n        super().__init__()\n        self.emb_layer = nn.Embedding(vocab_size, input_size)\n        self.rnn = nn.LSTM(input_size, hidden_size, num_layers=num_layers)\n        self.mlp = nn.Linear(hidden_size, vocab_size)\n        \n    def forward(self, x):\n        x = self.emb_layer(x) # TO embeddings (batch_size, seq_len, input_size)\n        rnn_output, h = self.rnn(x) # Through RNN (batch_size, seq_len, hidden_size)\n        return self.mlp(rnn_output)\n\n\nlm = LM()\ntokeized_tune = torch.tensor([tokenizer.encode(dataset[0]['Tune'])])\nx = tokeized_tune[:,:-1] # All but the last token\ny = tokeized_tune[:,1:] # All but the first token (x shifted by 1)\nx.shape, lm(x).shape, y.shape\n\n(torch.Size([1, 105]), torch.Size([1, 105, 200]), torch.Size([1, 105]))\n\n\n\ntorch.tensor(tokenizer(dataset['Tune'][:10], padding=True)['input_ids']).shape\n\ntorch.Size([10, 339])\n\n\n\ndef lm_loss_function(model_pred, y):\n    b, n, vc = model_pred.shape\n    pred = model_pred.reshape(b*n, vc)\n    target = y.flatten()\n    return nn.functional.cross_entropy(pred, target)\n\nlm_loss_function(lm(x), y)\n\ntensor(5.2998, grad_fn=<NllLossBackward0>)\n\n\n\nfrom datasets import Dataset\n\n\ndataset = load_dataset('tglcourse/abc_tunes', split='train').shuffle()\nmax_length = 201\ntokenized_tunes = tokenizer(dataset['Tune'], padding=True, truncation=True, max_length=max_length)['input_ids']\nlm_dataset = Dataset.from_dict({\n    'x':torch.tensor([t[:-1] for t in tokenized_tunes]),\n    'y':torch.tensor([t[1:] for t in tokenized_tunes]),\n})\nlm_dataloader = torch.utils.data.DataLoader(lm_dataset.with_format(\"torch\"), batch_size=256)\n\nUsing custom data configuration tglcourse--abc_tunes-5a89386c12e016f6\nReusing dataset parquet (/root/.cache/huggingface/datasets/tglcourse___parquet/tglcourse--abc_tunes-5a89386c12e016f6/0.0.0/7328ef7ee03eaf3f86ae40594d46a1cec86161704e02dd19f232d81eee72ade8)\n\n\n\nbatch = next(iter(lm_dataloader))\nbatch['x'].shape, batch['y'].shape\n\n(torch.Size([256, 200]), torch.Size([256, 200]))\n\n\n\nfrom tqdm.notebook import tqdm\n\n\nlm = LM().cuda()\noptimizer = torch.optim.Adam(lm.parameters(), lr=1e-4)\nlosses = []\nfor epoch in range(10):\n    for batch in tqdm(lm_dataloader):\n        x = batch['x'].cuda()\n        y = batch['y'].cuda()\n        model_preds = lm(x)\n        loss = lm_loss_function(lm(x), y)\n        loss.backward()\n        losses.append(loss.item())\n        optimizer.step()\n        optimizer.zero_grad()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(losses)\n\n\n\n\n\nimport numpy as np\ndef generate(model, start_text, n, max_length=200, top_k=None):\n    model.eval()\n    tokenized_start = torch.tensor([tokenizer(start_text)['input_ids']]).cuda()\n    x = tokenized_start\n    for i in range(n):\n        model_pred = model(x)\n        last_word_logits = model_pred[0][0]\n        p = torch.nn.functional.softmax(last_word_logits, dim=0).data\n        \n        # Set p to zero for special tokens we don't want\n        p[:4] = 0\n        \n        # Sample\n        if top_k is None:\n            candidate_tokens = np.arange(len(last_word_logits))\n        else:\n            p, candidate_tokens = p.topk(top_k)\n            candidate_tokens = candidate_tokens.detach().cpu().numpy().squeeze()\n        p = p.detach().cpu().numpy().squeeze()\n        word_index = np.random.choice(candidate_tokens, p=p/p.sum())\n        x = torch.cat([x, torch.tensor([[word_index]]).cuda()], dim=-1)\n        \n    return tokenizer.decode(x.flatten())\ngenerate(lm, 'B|F', 20)\n\n'B | F b Z X, c : Y º B l [ B'\n\n\n\ngenerate(lm, 'B|F', 200,top_k=20)\n\n'B | F 2 e F 3 ( ( a / A c E A E \" \" a d a g c E D :   A c   B F   f G : / d   a c A : f D E ( a \" e 2 D 2 F 2 3 E \" c a f 2 2 A G \" d E / E G c 2 / D a | 2 B D f B B   g e c e d B e g   ( f ( D g 3 G e : c g g g ( : | 2 2 / : a \" F G c 3 f \" F / | f \" c E 3 F a g F | f e G 2 D f E d E f a g G ( 3 c g ( G / E F 3 : ( \" 2 3 2 ( e G G E c / A g : / B e a / /   d : A f c / : E 2 \" f : 3 G | / | d | / F \" F'\n\n\n\nsum([p.numel() for p in lm.parameters()])\n\n247240\n\n\n\n# TODO try on some data (LM first then new classification head)\n\n\n# TODO talk about efficiency of training vs sampling\n# TODO demo different sampling approaches\n\nPage stats: Total Hits:  Page visitors:"
  },
  {
    "objectID": "generators_and_losses.html",
    "href": "generators_and_losses.html",
    "title": "Fun with Generators and Losses",
    "section": "",
    "text": "This content is adapted from a talk I gave to the MIT ‘Computer Visions’ class. I’ll link the recording if that get’s released, but in the meantime here is a different variant I recorded that includes a notebook run-through which should explain most of the code here.\nWe spoke about how deep learning relies heavily on one key idea: optimization:\nWe’re going to apply this mindset to the task of creating imagery in creative ways. To this end, we’ll explore a number of different ‘generators’ (each of which create an image from some set of parameters) and a number of ‘losses’ (which try to measure how ‘good’ the generated images are by some measure). And then we’ll play with combining different generators and losses to achieve different outputs.\nThese will be introduced one by one, but I’ve tried to make them as interchangeable as possible so that you can swap in or combine any of these building blocks for any of the demos. And at the end there’s a template for you to build your own final custom image generation tool and some hints for ideas to explore."
  },
  {
    "objectID": "generators_and_losses.html#our-first-generator-raw-pixels",
    "href": "generators_and_losses.html#our-first-generator-raw-pixels",
    "title": "Fun with Generators and Losses",
    "section": "Our First Generator: Raw Pixels",
    "text": "Our First Generator: Raw Pixels\nWhat if we just optimize some pixels directly? An image is now represented by a number of parameters (the raw RGB values). This should be a good test case, and a chance to think about how we want to frame our Generators going forward.\nWe need access to the parameters we can tweak, and a way to get the output.\nThe best way I know of is to lean on the machinery PyTorch has for neural networks by inheriting from the nn.Module class. nn.Parameter() makes a tensor that automatically has gradient tracking set up, and all parameters created this way can be accessed with the parameters() function of our generator - which saves us needing to write that ourselves.\nWe specify how we’d like to produce an output by defining the forward method. This lets us use our new object as a function - when we run im = gen() we’re actually saying im = gen.forward().\nThis might seem slightly overkill for this first example, but let’s just check it out and see how it works:\n\ngen = PixelGenerator(128)\nim = gen() # Get the output of the generator (the image)\nprint(f'Output shape: {im.shape}')\ntensor_to_pil(im) # View it\n\nOutput shape: torch.Size([1, 3, 128, 128])\n\n\n\n\n\nInspecting the parameters:\n\n[p.shape for p in gen.parameters()]\n\n[torch.Size([1, 3, 128, 128])]\n\n\nThere we go. Hopefully this will become useful in a second."
  },
  {
    "objectID": "generators_and_losses.html#our-first-loss-mean-squared-error",
    "href": "generators_and_losses.html#our-first-loss-mean-squared-error",
    "title": "Fun with Generators and Losses",
    "section": "Our First Loss: Mean Squared Error",
    "text": "Our First Loss: Mean Squared Error\nWe’ll take the difference between an image and a target and square it.\n\n# Make a target image\ntarget_image = torch.zeros(1, 3, 128, 128)\ntarget_image[:,1] += 1 # Set the green channel to all ones\ntensor_to_pil(target_image) # View it\n\n\n\n\n\n# Create a loss function with this as the target\nmse_loss = MSELossToTarget(target_image, size=128)\n\n\n# Calculate the loss between this and the output of a generator\ngen = PixelGenerator(128)\nim = gen()\nmse_loss(im) # We get a single measure with a way to trace the gradients backward\n\ntensor(0.3330, grad_fn=<MeanBackward0>)\n\n\nQ: Does that number make sense? What would the theoretical prediction be?"
  },
  {
    "objectID": "generators_and_losses.html#optimization",
    "href": "generators_and_losses.html#optimization",
    "title": "Fun with Generators and Losses",
    "section": "Optimization",
    "text": "Optimization\nWe want to tweak the parameters of our generator to make the loss (derived from the output) lower. Here’s how we might do this in PyTorch:\n\n# Set a target - here a green image as in the previous example\ntarget_image = torch.zeros(1, 3, 128, 128)\ntarget_image[:,1] += 1\n\n# Make a loss function based on this target\nmse_loss = MSELossToTarget(target_image, size=128)\n\n# Set up our generator\ngen = PixelGenerator(128)\n\n# Set up an optimizer on the generators parameters\noptimizer = torch.optim.Adam(gen.parameters(), lr=1e-2)\n\n\n# get the generator output\nim = gen()\n\n# find the loss\nloss = mse_loss(im)\n\n# Reset any stored gradients\noptimizer.zero_grad()\n\n# Calculate the gradients\nloss.backward()\n\n# Print the loss\nprint(loss.item()) \n\n# Update the generator parameters to reduce this loss\noptimizer.step()\n\n0.335597962141037\n\n\nRe-run the above cell a number of times, and use the following cell to see the current output:\n\ntensor_to_pil(gen()) # Generate and view an image\n\n\n\n\nIt gets greener over time - and the loss goes down. Hooray! Let’s define some new generators and loss functions and then make a clean version of this optimization code that runs in a loop so we don’t need to keep re-running a cell!"
  },
  {
    "objectID": "generators_and_losses.html#next-generator-imstack",
    "href": "generators_and_losses.html#next-generator-imstack",
    "title": "Fun with Generators and Losses",
    "section": "Next Generator: ImStack",
    "text": "Next Generator: ImStack\nImStack is a library I made to represent images as a ‘stack’ of tensors of different sizes. The intuition here is that the lowest level can incorporate the large shapes and higher layers can capture fine details. When optimizing it can be useful to have a few parameters that have a large effect on the output - this can allow a cleaner gradient signal than if each pixel is independant.\nThis ‘generator’ just wraps an imstack. Note that it is the same as the pixel_generator except that we have a few extra possible arguments when creating one - for example we can initialise it with an input image (which we’ll try soon).\n\ngen = ImStackGenerator(size=128, n_layers=4, base_size=16)\nim = gen()\nprint(f'Output shape: {im.shape}')\nprint(f'Parameter shapes: {[p.shape for p in gen.parameters()]}')\ntensor_to_pil(im)\n\nOutput shape: torch.Size([1, 3, 128, 128])\nParameter shapes: [torch.Size([3, 16, 16]), torch.Size([3, 32, 32]), torch.Size([3, 64, 64]), torch.Size([3, 128, 128])]\n\n\n\n\n\nBreaking down the layers in the stack:\n\ngen.imstack.plot_layers()\n\n\n\n\nYou can explore tweaking the base_size, n_layers and scale parameters to see how they affect the look of the initial (random) output and the total number of parameters."
  },
  {
    "objectID": "generators_and_losses.html#style-transfer",
    "href": "generators_and_losses.html#style-transfer",
    "title": "Fun with Generators and Losses",
    "section": "Style Transfer",
    "text": "Style Transfer\nNow we’re going to try a classic application of pretrained models for artistic purposes: style transfer.\n\nExtracting features from a pretrained model\nPytorch has definitions of many common model architectures, and ways for loading pre-trained versions of them. In this case, we go for a small, older architecture called VGG16 trained on Imagenet (a dataset with >1M across 1k classes):\n\n# Load a pretrained model:\nvgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1).to(device)\nvgg16.eval()\nvgg16 = vgg16.features\n\n\nim = torch.rand(1, 3, 128, 128).to(device) # A random image for demo\nfeats = calc_vgg_features(vgg16, im) # The activations of the specified layers\n[f.shape for f in feats] # See the shapes of the returned features\n\n[torch.Size([1, 3, 16384]),\n torch.Size([1, 64, 16384]),\n torch.Size([1, 128, 4096]),\n torch.Size([1, 256, 1024]),\n torch.Size([1, 512, 256]),\n torch.Size([1, 512, 64])]\n\n\nYou can see that from an input image we’ve got a bunch of features, one for each specified layer. We will use these for the style and content losses.\n\n\nContent Loss/Perceptual Loss\nRemember our picture of a CNN: \nWe spoke about how early layers tend to capture edges and textures, while later layers aggregate these smaller features into more complex ones.\nWe can exploit this to try and focus on the broad ‘content’ of an image in a way that is robust to small changes to texture or color. To achieve this, we’ll look only at activations from some deeper layers, in this case specified by content_layers = [14, 19]. You can print the network description and pick a few - see how changing them affects things!\n\n# print(vgg16)\n\n\ncontent_loss = ContentLossToTarget(im)\ncontent_loss(torch.rand(1, 3, 64, 64).to(device))\n\ntensor(2.4038, device='cuda:0', grad_fn=<DivBackward0>)\n\n\n\n# TODO: handle VGG16 when in other notebooks!\n\nWe won’t do a demo with just this loss, but feel free to experiment with it after you’ve seen a few of the upcoming demos. What happens when you start from random noise and optimise with just content loss to a target image - does it perfectly re-produce the target? What about intermediate stages - what kinds of feature appear first?\n\n\nStyle Loss (OT version)\nIn a similar way, we want to capture style features. We mentioned that these will be better described by earlier layers, but there is a hitch: we want the styles of a target image, but not necessarily in the same places (otherwise we’d just get the whole picture!). So we need some way to remove the spatial component and just focus on the relative mix of colours, textures etc.\nThere are a few approaches. Most tutorials will use a gram-matrix based approach (which works fine) but I recently heard of a potentially better approach using ideas of optimal transport via this great video. We’ll implement both and you can compare the two for yourself :)\nBoth give super large loss figures by default, so I’ve included a scale_factor argument to tame the values a little.\n\n# Create and test a version of this loss\nstyle_loss = OTStyleLossToTarget(im)\nstyle_loss(torch.rand(1, 3, 64, 64).to(device))\n\ntensor(10.4530, device='cuda:0', grad_fn=<MulBackward0>)\n\n\n\n# Testing...\nstyle_loss = GramStyleLossToTarget(im, vgg16=vgg16)\nstyle_loss(torch.rand(1, 3, 64, 64).to(device))\n\ntensor(10.6278, device='cuda:0', grad_fn=<MulBackward0>)\n\n\n\n# Create and test a version of this loss\nstyle_loss = VincentStyleLossToTarget(im, vgg16=vgg16)\nstyle_loss(torch.rand(1, 3, 128, 128).to(device))\n\ntensor([[5.7033]], device='cuda:0', grad_fn=<MulBackward0>)"
  },
  {
    "objectID": "generators_and_losses.html#new-generator-siren",
    "href": "generators_and_losses.html#new-generator-siren",
    "title": "Fun with Generators and Losses",
    "section": "New Generator: SIREN",
    "text": "New Generator: SIREN\nSIREN represents an image in an interesting way, using a bunch of sinusiodal functions in a network. Anyone with some signals processing background can probably guess why this seems interesting.\nWe’ll wrap a library that does all the hard work for us, but just for curiosity’s sake we can at least look at the building blocks, starting with the activation function:\n\ngen = SirenGenerator(size=128)\nim = gen()\nprint(f'Output shape: {im.shape}')\nprint(f'Parameter shapes: {[p.shape for p in gen.parameters()]}')\ntensor_to_pil(im)\n\nOutput shape: torch.Size([1, 3, 128, 128])\nParameter shapes: [torch.Size([64, 2]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([3, 64]), torch.Size([3])]\n\n\n\n\n\n\n# TODO copy in and explain the rest of the code from https://colab.research.google.com/drive/1bsboh2GCxUwdzSmSg9AeCEaKKfW-hd74#scrollTo=wh8JQDX4izqN?\n\nWhat is neat here is that the output of the network is a function of x and y coords - we can evalluate this function at any resolution! No nasty pixels here. We can also control the number of parameters by chanigng the number and size of the layers. For example, here are two versions and the corresponding total number of parameters:\n\n# The default\ngen = SirenGenerator()\nprint('Number of parameters in default net:', sum([p.numel() for p in gen.parameters()]))\n\n# A smaller version\ngen = SirenGenerator(dim_hidden=16, num_layers=3)\nprint('Number of parameters in mini version:', sum([p.numel() for p in gen.parameters()]))\n\nNumber of parameters in default net: 17027\nNumber of parameters in mini version: 643\n\n\nSince these networks can be quite small, and run once per pixel at whatever size you want to generate, they are perfect for running as compute shaders. For eg, I trained a SIREN network with CLIP and turned it into a shader here: https://www.shadertoy.com/view/flGSDD (animating some of the parameters for a cool effect)."
  },
  {
    "objectID": "generators_and_losses.html#final-generator-bokeh",
    "href": "generators_and_losses.html#final-generator-bokeh",
    "title": "Fun with Generators and Losses",
    "section": "Final Generator: Bokeh!",
    "text": "Final Generator: Bokeh!\nI’m going to show one final generator here as a demo of how you can get more creative with things like this. I wanted to make images with a small number of shapes, and while playing around got the idea of summing gaussians to get blurry blobs of different colours.\nWhat are the parameters? The location, color, intensity and size of eah blob.\nHow do we render this in a way that is differentiable? It’s a little tricky, but to make it easier I did something to make any deep learning researcher cringe: I wrote a for loop. As in, for each dot: ... We don’t like things like this because GPUs are good at doing things in parallel! But hacky as it is, it works! You don’t have to do everything perfectly ;)\nThis code isn’t itself very interesting or worth copying, but hopefully it does highlight the more general idea: hack things together and have fun!\n\n# Create one with 100 blobs\nd = DotGenerator(100)\n\ngen = DotGenerator(size=256)\nim = gen()\nprint(f'Output shape: {im.shape}')\nprint(f'Parameter shapes: {[p.shape for p in gen.parameters()]}')\ntensor_to_pil(im)\n\nOutput shape: torch.Size([1, 3, 256, 256])\nParameter shapes: [torch.Size([2, 100]), torch.Size([100]), torch.Size([100]), torch.Size([3, 100]), torch.Size([100])]\n\n\n\n\n\nYou’ll need to tweak parameters to keep the image looking nice with larger sizes or different numbers of dots, but at least this does roughly what we wanted. Inpecting the parameters you’ll see we have a few for each dot (100 dots here):\nhttps://teia.art/sparkles_jw has examples of some animations made with this same idea…"
  },
  {
    "objectID": "generators_and_losses.html#final-loss-clip",
    "href": "generators_and_losses.html#final-loss-clip",
    "title": "Fun with Generators and Losses",
    "section": "Final Loss: CLIP",
    "text": "Final Loss: CLIP\nOK, the final loss function is going to feel like a super-power. What if we want to just describe what we want in text?\nEnter CLIP. Remember: CLIP maps images and text to the same space, so we can compare them. We’ll load a CLIP model and test this for ourselves for a few mini demos before turning this into another loss function we can use to guide generation.\nText and image similarity (show use for one-shot classification and search)\nText or image (or multiple) as prompts\n\n#@title load a clip model\n\n# A nice small model (B='base') - good for quick tests and smaller download:\nclip_model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32-quickgelu', pretrained='laion400m_e32')\n\n# A medium one (L='large'):\n# clip_model, _, preprocess = open_clip.create_model_and_transforms('ViT-L-14', pretrained='laion2b_s32b_b82k')\n\n# A massive one (H='huge') that needs lots of RAM but might generate better images?:\n# model, _, preprocess = open_clip.create_model_and_transforms('ViT-H-14', pretrained='laion2b_s32b_b79k')\n\n\n# print(preprocess)\npreprocess = T.Compose([\n    T.Resize(size=224, max_size=None, antialias=None),\n    T.CenterCrop(size=(224, 224)),\n    T.Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n])\nclip_model.to(device)\n\n# We don't want to train CLIP at all so setting requires_grad=False everywhere\n# Probably unnecessary but rather safe than sorry :)\nclip_model.eval();\nfor p in clip_model.parameters():\n  p.requires_grad = False\n\n\n# One shot classification demo\n# Load an image\ncat_im = pil_to_tensor(Image.open('cat.jpeg')).to(device)\n\n# Encode the image with CLIP\nimage_embed = clip_model.encode_image(preprocess(cat_im))\nprint('Image embed shape:', image_embed.shape)\n\n# Encode some labels with CLIP\ntokenized_text = open_clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(device)\ntarget_embeds = clip_model.encode_text(tokenized_text)\nprint('Texts embed shape:',target_embeds.shape) # One for each label\n\n# Find the similarity to each \ntorch.nn.CosineSimilarity()(image_embed, target_embeds)\n\nImage embed shape: torch.Size([1, 512])\nTexts embed shape: torch.Size([3, 512])\n\n\ntensor([0.1011, 0.1522, 0.2708], device='cuda:0')\n\n\nWe see a higher similarity for the label ‘a cat’ vs ‘a dog’ and ‘a diagram’ is the lowest’\nWe can flip this around to do image search. Given a load of images, we embed a text query and find the image that is the best match. This could be a fun exercise to try ;)\n\nUsing it as a loss\nWe can look at the similarity between the CLIP embedding of a generated image and one or more CLIP embeddings of images or text we’re feeding in as targets.\nLet’s look at this in action:\n\n# Create a generator and get an output im\ngen = SirenGenerator(size=128).to(device)\ngen.to(device)\nim = gen()\n\n# Embed this with CLIP\nwith torch.no_grad():\n  image_embed = clip_model.encode_image(preprocess(im))\nprint(image_embed.shape)\n\n# Embed some target texts\nwith torch.no_grad():\n  tokenized_text = open_clip.tokenize([\"a blue cat\", \"A cat picture\"]).to(device)\n  target_embeds = clip_model.encode_text(tokenized_text)\nprint(target_embeds.shape)\n\n# I wrote clip_loss_embeddings to take an image embed and multiple target embeds,\n# and return the average loss across the different targets:\nclip_loss_embeddings(image_embed, target_embeds)\n\ntorch.Size([1, 512])\ntorch.Size([2, 512])\n\n\ntensor(1.0452, device='cuda:0')\n\n\nMaking our neat loss class It helps to make multiple variations of the generated image so CLIP doesn’t see the exact same thing each time - hence the make_cutouts bit here. More cutouts => cleaner loss signal but more memory usage. You can explore this or just go with the defaults.\n\n# Testing...\nclip_loss_fn = CLIPLossToTargets(text_prompts=['A cat'], image_prompts=[im], clip_model=clip_model)\nclip_loss_fn(torch.rand(1, 3, 64, 64).to(device))\n\ntensor(0.5813, device='cuda:0')"
  },
  {
    "objectID": "generators_and_losses.html#ideas",
    "href": "generators_and_losses.html#ideas",
    "title": "Fun with Generators and Losses",
    "section": "Ideas",
    "text": "Ideas\nCan you get cool-looking images with a really small SIREN network? Can you get a nice-looking style transfer demo working? Can you implement a loss function that forces a specific palette of colours? Or a color gradient across the image? Can you chain these, first optimising towards one target then another (hint: just re-use a generator and call optimise again with different loss functions!). For eg SIREN to an image (via MSE) then to a CLIP prompt to tweak it. Advanced: Can you add new generators? One based on ‘Deep Image Priors’ perhaps, or using something like VQGAN and optimizing the latents which are then decoded by the VQGAN decoder to get an output image. Can you think of a loss that would avoid noisy images? Hint: look up ‘TV Loss’ How about a loss that penalizes over-saturated colours? How about a generator that can only make black and white images? How would you make a generator that created ‘seamless’ images to use in combination with CLIP? Advanced: What about using PyTorch3D’s differentiable rendering to optimise the vertices of a shape or the texture of a 3d model to match a style or CLIP prompt?\nPage stats: Total Hits:  Page visitors:"
  },
  {
    "objectID": "gan1.html",
    "href": "gan1.html",
    "title": "Lesson 7: GANs Part 1 - GAN Training",
    "section": "",
    "text": "# TODO video intro"
  },
  {
    "objectID": "gan1.html#gans---theory",
    "href": "gan1.html#gans---theory",
    "title": "Lesson 7: GANs Part 1 - GAN Training",
    "section": "GANs - Theory",
    "text": "GANs - Theory\nThe motivation for GANs was the desire for a generative model that could generate plausible new images (or any kind of data for that matter) that look like they could have come from some domain or training set. In the previous lesson we saw how Variational Auto-Encoders learn how to encode an input into some latent representation and then decode it back into an output, and how the decoder part of an auto-encoder can be used to generate new images.\nGANs achieve a similar result but tend to peform far better than auto-encoder based systems at generating new, novel outputs.\nThey do this by training two separate sub-networks, a generator network and a discriminator network. They are trained in parallel. The goal of the discriminator is to tell whether a given image is real (aka from the training data) or fake (aka generated by the generator model). The generator tries to create plausible outputs to fool the discriminator.\n\n# TODO images\n\nA typical GAN training loop might look something like the following:\nfor batch in data:\n\ngenerate a batch of fake images\nfeed both the fake images and the batch of training images into the discriminator\ncalculate the discriminator loss and update the discriminator weights to improve it’s accuracy\nupdate the weights of the generator to better fool the discriminator (i.e. increase the discriminator loss)\n\nThis core idea is very simple, and yet GAN training can fail in some unexpected ways. For example, a GAN that perfectly memorizes the training data can fool the discriminator perfectly, but might not be able to generate any new data. Even worse: it can get away with memorizing only a subset of the training data, a situation called ‘mode collapse’.\nBecause of these quirks, GAN training is viewed by many as some sort of dark art! But in this lesson we’re going to face it bravely, exploring a basic GAN implementation and seeing for ourselves what training looks like. In the next lesson, we’ll introduce some additional ideas and a few of the tricks people use to get better results with GANs. And in the bonus notebook [coming some time] we’ll look at an idea called NO-GAN.\n“Wait, I just want to train a good GAN right now!” you say? Fair enough - in that case skip this simple demo and jump straight to something like LightWeightGAN which has ready-to-use training scripts, or look around for one of the many StyleGAN tutorials."
  },
  {
    "objectID": "gan1.html#dc-gan-from-scratch",
    "href": "gan1.html#dc-gan-from-scratch",
    "title": "Lesson 7: GANs Part 1 - GAN Training",
    "section": "DC-GAN From Scratch",
    "text": "DC-GAN From Scratch\nMuch of the code here comes from the PyTorch docs [TODO link]\n\nxb = next(iter(train_dataloader))['images'].to(device)[:8]\n\n\ndataset = load_dataset(\"huggan/smithsonian_butterflies_subset\", split=\"train\")\n\nimage_size = 32\nbatch_size = 32\n\npreprocess = transforms.Compose(\n    [\n        transforms.Resize((image_size, image_size)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.5], [0.5]),\n    ]\n)\n\ndef transform(examples):\n    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n    return {\"images\": images}\n\ndataset.set_transform(transform)\n\ntrain_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
  },
  {
    "objectID": "gan1.html#a-few-improvements",
    "href": "gan1.html#a-few-improvements",
    "title": "Lesson 7: GANs Part 1 - GAN Training",
    "section": "A few improvements",
    "text": "A few improvements\n\nAdd logging to training loop\nAdd a few things like pixelshuffle/blur\nLeave space to tweak loss functions and such\nExport a bunch of components for next notebook"
  },
  {
    "objectID": "gan1.html#playing-with-an-existing-gan",
    "href": "gan1.html#playing-with-an-existing-gan",
    "title": "Lesson 7: GANs Part 1 - GAN Training",
    "section": "Playing with an existing GAN",
    "text": "Playing with an existing GAN\n\n# Load a lightweight-GAN? Or a model trained with the above code (wrapped in a script) for a little longer?\n\n\n# latent walk\n\n\n# Guide with a loss (demo one from generators and losses notebook\n\n\n# Link to butterflyGAN demo on HF spaces\n\nPage stats: Total Hits:  Page visitors:"
  },
  {
    "objectID": "sequence_3.html",
    "href": "sequence_3.html",
    "title": "Lesson 11: Everything Is A Sequence",
    "section": "",
    "text": "Page stats: Total Hits:  Page visitors:"
  },
  {
    "objectID": "discussions.html",
    "href": "discussions.html",
    "title": "Discussions",
    "section": "",
    "text": "Most of this course is me (Jonathan Whitaker) talking into my webcam. I do my best to cover the different topics we explore in this course, but you’re ultimately getting a single viewpoint on all of this - not ideal! There are thousands of people with unique backgrounds and perspectives working in this space, and this discussions series is an attempt to bring in some of those different voices and ideas. If there is someone you think should be on this list, please reach out and let me know! I’m still figuring out the recording workflow, so some early interviews might have slightly sub-optimal audio - I’ll add transcripts ASAP."
  },
  {
    "objectID": "discussions.html#apolinário-passos---ml-art-engineer-huggingface",
    "href": "discussions.html#apolinário-passos---ml-art-engineer-huggingface",
    "title": "Discussions",
    "section": "Apolinário Passos - ML Art Engineer @ Huggingface",
    "text": "Apolinário Passos - ML Art Engineer @ Huggingface\nApolinário is one of the best folks to follow to keep track of what is happening in this space. His ‘MindsEye’ tool and more recently the ‘Majesty Diffusion’ notebook have helped to make some of the latest AI art techniques easier to use for non-technical users. These days he works at HuggingFace in the intersection between the technical side and the artistic community. In this conversation we discuss his journey and work, how to stay on top of new releases and how to get involved.\n\n\n\n        \n        \n\n\nLinks:  + Apolinário’s website: multimodal.art/  + His @multimodalart Twitter account"
  },
  {
    "objectID": "discussions.html#enzymezoo---artist-and-developer-deforum",
    "href": "discussions.html#enzymezoo---artist-and-developer-deforum",
    "title": "Discussions",
    "section": "@EnzymeZoo - Artist and Developer (Deforum)",
    "text": "@EnzymeZoo - Artist and Developer (Deforum)\n@EnzymeZoo is an artist and developer. In this discussion we chat about the community development work that goes into creating a tool like the Deforum notebook. We explore how that project came together, how you can get involved and @EnzymeZoos general thoughts on AI art and creativity.\n\n\n\n        \n        \n\n\nLinks:  + Deforum Discord (user discord, but from there you can find the developer group as well)  + Jax diffusion notebook  + Disco Diffusion notebook  + List of tools by @pharmapsychotic which has many more notebooks and guides: https://pharmapsychotic.com/tools.html  + An artist making very cool animations with Deforum that @EnzymeZoo shared: @Infinite Vibes  + More creations using deforum on Twitter \nAfter we stopped recording @EnzymeZoo also asked that I mention/thank Stability AI. The release of Stable Diffusion was the catalyst for Deforum springing into existence, and they have since started supporting some of the developers behind this and other notebooks."
  },
  {
    "objectID": "discussions.html#teodora-szasz",
    "href": "discussions.html#teodora-szasz",
    "title": "Discussions",
    "section": "Teodora Szasz",
    "text": "Teodora Szasz\nTeodora works at the University of Chicago helping researchers from different departments incorporate deep learning into their work. She also runs workshops on topics like AI Art and creative coding! In this discussion we chat about her background and research, then zero in on her research into representation in media and how generative models might help create stories with characters that more people can identify with.\n\n\n\n        \n        \n\n\nLinks:  + A video on the childrens book project  + Teodora’s Twitter @TeodoraSzasz  + https://github.com/joojs/fairface"
  },
  {
    "objectID": "discussions.html#jason-antic",
    "href": "discussions.html#jason-antic",
    "title": "Discussions",
    "section": "Jason Antic",
    "text": "Jason Antic\nJason is the creator of the incredible ‘DeOldify’ application for restoring historical photos, and one of the best experimentalists I’ve met. In this conversation we chat a little bit about his journey then dive into the nuts and bolts of what research actually looks like. Audio only since I lost power as recording started and didn’t notice the video recording freezing.\n\n\n\n        \n        \n\n\nLinks:  + Deoldify integrated into MyHeritage: https://www.myheritage.com/incolor  + A (slightly outdated) blog post with some technical details  + The fast.ai course that we both credit as a large part of our paths into deep learning: https://course.fast.ai/"
  },
  {
    "objectID": "discussions.html#hamel-husain",
    "href": "discussions.html#hamel-husain",
    "title": "Discussions",
    "section": "Hamel Husain",
    "text": "Hamel Husain\nHamel is a Machine Learning Engineer/Data Scientist passionate about making tools. He is one of the most helpful and interesting people I’ve met. He turned the usual interview process on its head and decided to interview me too, so you get two for the price of one in this episode!\n\n\n\n        \n        \n\n\nLinks: + Hamel’s personal site: hamel.dev  + NBDev\nPage stats: Total Hits:  Page visitors:"
  },
  {
    "objectID": "dm1.html",
    "href": "dm1.html",
    "title": "An Introduction to Diffusion Models",
    "section": "",
    "text": "The next 3 lessons are a collaboration with Hugging Face and form the Hugging Face Diffusion Models Course. You can view the introduction to each Unit (lesson) here or on GitHub and the links to the notebooks load them from the Hugging Face repository so that they’re always up-to-date. In this lesson, you will learn the basics of how diffusion models work and how to create your own using the 🤗 Diffusers library."
  },
  {
    "objectID": "dm1.html#get-started",
    "href": "dm1.html#get-started",
    "title": "An Introduction to Diffusion Models",
    "section": "Get Started",
    "text": "Get Started\n\nSign up for the Hugging Face course so that you can be notified when new material is released\nRead through the introductory material below as well as any of the additional resources that sound interesting\nCheck out the Introduction to Diffusers notebook below to put theory into practice with the 🤗 Diffusers library\nTrain and share your own diffusion model using the notebook or the linked training script\n(Optional) Dive deeper with the Diffusion Models from Scratch notebook if you’re interested in seeing a minimal from-scratch implementation and exploring the different design decisions involved\n(Optional) Check out the video below for an informal run-through the material for this unit."
  },
  {
    "objectID": "dm1.html#what-are-diffusion-models",
    "href": "dm1.html#what-are-diffusion-models",
    "title": "An Introduction to Diffusion Models",
    "section": "What Are Diffusion Models?",
    "text": "What Are Diffusion Models?\nDiffusion models are a relatively recent addition to a group of algorithms known as ‘generative models’. The goal of generative modeling is to learn to generate data, such as images or audio, given a number of training examples. A good generative model will create a diverse set of outputs that resemble the training data without being exact copies. How do diffusion models achieve this? Let’s focus on the image generation case for illustrative purposes.\n\n   Figure from DDPM paper (https://arxiv.org/abs/2006.11239). \n\nThe secret to diffusion models’ success is the iterative nature of the diffusion process. Generation begins with random noise, but this is gradually refined over a number of steps until an output image emerges. At each step, the model estimates how we could go from the current input to a completely denoised version. However, since we only make a small change at every step, any errors in this estimate at the early stages (where predicting the final output is extremely difficult) can be corrected in later updates.\nTraining the model is relatively straightforward compared to some other types of generative model. We repeatedly 1) Load in some images from the training data 2) Add noise, in different amounts. Remember, we want the model to do a good job estimating how to ‘fix’ (denoise) both extremely noisy images and images that are close to perfect. 3) Feed the noisy versions of the inputs into the model 4) Evaluate how well the model does at denoising these inputs 5) Use this information to update the model weights\nTo generate new images with a trained model, we begin with a completely random input and repeatedly feed it through the model, updating it each time by a small amount based on the model prediction. As we’ll see, there are a number of sampling methods that try to streamline this process so that we can generate good images with as few steps as possible.\nWe will show each of these steps in detail in the hands-on notebooks here in unit 1. In unit 2, we will look at how this process can be modified to add additional control over the model outputs through extra conditioning (such as a class label) or with techniques such as guidance. And units 3 and 4 will explore an extremely powerful diffusion model called Stable Diffusion, which can generate images given text descriptions."
  },
  {
    "objectID": "dm1.html#hands-on-notebooks",
    "href": "dm1.html#hands-on-notebooks",
    "title": "An Introduction to Diffusion Models",
    "section": "Hands-On Notebooks",
    "text": "Hands-On Notebooks\nAt this point, you know enough to get started with the accompanying notebooks! The two notebooks here come at the same idea in different ways.\n\n\n\nChapter\nColab\nKaggle\nGradient\nStudio Lab\n\n\n\n\nIntroduction to Diffusers\n\n\n\n\n\n\nDiffusion Models from Scratch\n\n\n\n\n\n\n\nIn Introduction to Diffusers, we show the different steps described above using building blocks from the diffusers library. You’ll quickly see how to create, train and sample your own diffusion models on whatever data you choose. By the end of the notebook, you’ll be able to read and modify the example training script to train diffusion models and share them with the world! This notebook also introduces the main exercise associated with this unit, where we will collectively attempt to figure out good ‘training recipes’ for diffusion models at different scales - see the next section for more info.\nIn Diffusion Models from Scratch, we show those same steps (adding noise to data, creating a model, training and sampling) but implemented from scratch in PyTorch as simply as possible. Then we compare this ‘toy example’ with the diffusers version, noting how the two differ and where improvements have been made. The goal here is to gain familiarity with the different components and the design decisions that go into them so that when you look at a new implementation you can quickly identify the key ideas."
  },
  {
    "objectID": "dm1.html#project-time",
    "href": "dm1.html#project-time",
    "title": "An Introduction to Diffusion Models",
    "section": "Project Time",
    "text": "Project Time\nNow that you’ve got the basics down, have a go at training one or more diffusion models! Some suggestions are included at the end of the Introduction to Diffusers notebook. Make sure to share your results, training recipes and findings with the community so that we can collectively figure out the best ways to train these models."
  },
  {
    "objectID": "dm1.html#some-additional-resources",
    "href": "dm1.html#some-additional-resources",
    "title": "An Introduction to Diffusion Models",
    "section": "Some Additional Resources",
    "text": "Some Additional Resources\nThe Annotated Diffusion Model is a very in-depth walk-through of the code and theory behind DDPMs with maths and code showing all the different components. It also links to a number of papers for further reading.\nHugging Face documentation on Unconditional Image-Generation for some examples of how to train diffusion models using the official training example script, including code showing how to create your own dataset.\nAI Coffee Break video on Diffusion Models: https://www.youtube.com/watch?v=344w5h24-h8\nYannic Kilcher Video on DDPMs: https://www.youtube.com/watch?v=W-O7AZNzbzQ\nFound more great resources? Let us know and we’ll add them to this list.\nPage stats: Total Hits:  Page visitors:"
  },
  {
    "objectID": "dm4.html",
    "href": "dm4.html",
    "title": "Lesson 15: Diffusion for Audio",
    "section": "",
    "text": "Class conditioned birdcalls\nPage stats: Total Hits:  Page visitors:"
  },
  {
    "objectID": "optimization.html",
    "href": "optimization.html",
    "title": "Lesson 2: Gradient Descent and Optimization",
    "section": "",
    "text": "If you’d prefer a much longer video with more explanation, here is a fairly rough live notebook walkthrough."
  },
  {
    "objectID": "optimization.html#gradient-descent",
    "href": "optimization.html#gradient-descent",
    "title": "Lesson 2: Gradient Descent and Optimization",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n\n\n\n\n\nLets consider the following situation. Say we have: - A function (could be a neural net, could be the equation for a straight line…) that takes some input(s) and produces some output(s) based on a set of parameters - let’s call them w.  - Some measure of how well this function performs. Maybe this is how poorly the function describes some pattern in your data, or how well a network does in a classification task. Let’s call this measure the loss, where the goal is to make this as small as possible.\nThe question is often ‘how do we find a set of parameters that gives the best possible result?’. There are a few ways we could try to solve this. The most basic might be: - Try all possible values for all parameters - Randomly guess and keep the best\nClearly both of these have some major flaws, and when we’re dealing with thousands or millions of parameters there is no way you could try all possible combinations. So, we need a smarter approach.\n\nThe Gradient Descent Algorithm\nWhat if we could start from some set of parameters, and then see how to modify them slightly such that we get an improvement? Ideally, for each parameter we’d like to know what happens to the loss when we tweak that parameter slightly up or down. Formally, we’d like to know the gradient of the loss with respect to that parameter. You can think of the gradient as telling us which direction to move to get the biggest increase (or decrease if we go in the opposite direction).\nIF we can find these gradients, then a sensible method for finding a good set of parameters to solve a given problem would be 1. Start with some random parameters 2. Find the gradient of the loss with respect to each parameter 3. Update each parameter such that you move some small amount in the direction of steepest descent 4. Go back to step 2, finding the gradients based on the new parameter values and repeat all this a bunch of times.\nThis is the gradient descent algorithm in a nutshell :) Let’s do an example, where we’ll create some data that roughtly follows a trend and try to approximate that trend with a straight line, which will be specified by two parameters.\n\n\nCreating an Example Problem\nHere’s our ‘training’ data, with a single input (x) and a single target (y):\n\n# Creating some data:\nx = torch.rand(20)\ny = 3*x + 0.2 + torch.randn(20)*0.3 # y = ax + b + noise\nplt.scatter(x, y) # It's always helpful to visualize what's going on wherever possible.\n\n<matplotlib.collections.PathCollection>\n\n\n\n\n\n\n\n2.3 Defining our loss\nWe can describe a line as a function y = ax + b where a and b are our parameters. Take a look at the two lines shown here:\n\nfig, axs = plt.subplots(1, 2, figsize=(10, 3.6))\naxs[0].scatter(x, y)\naxs[0].plot(x, 0.1*x + 2.6, label='y1 = 0.1x + 2.6', c='orange')\naxs[0].legend()\naxs[1].scatter(x, y)\naxs[1].plot(x, 2*x + 0.5, label='y2 = 2*x + 0.5',  c='orange')\naxs[1].legend();\n\n\n\n\nClearly one does a better job than the other at describing the trend in this data. But how do we quantify this? There are several measures used in this sort of case, with a popular one being the ‘Root Mean Squared Error’. It sounds intimidating, but all we do is take the errors (how far each point is from the line), square them, sum the squares and then take the square root of that. More points further from the line -> higher errors (squaring takes care of any minus signs that would otherwise cause issues with points above/below the line) -> a higher final value for the RMSE. So lower is better. This is our loss function.\nHere’s one way to implement this in code (there are also built-in functions for this and many other ways you could write it):\n\ndef rmse(y, y_pred):\n    return torch.mean((y-y_pred)**2)**0.5 # See how many ways you can write this\n\nUsing this loss function, we can quantify how well those lines match the data:\n\ny1 = 0.1*x + 2.6\ny2 = 2*x + 0.5\nprint('RMSE for y_hat1 = 0.1x + 2.6:', rmse(y, y1))\nprint('RMSE for y_hat2 = 2*x + 0.5:', rmse(y, y2))\n\nRMSE for y_hat1 = 0.1x + 2.6: tensor(1.3639)\nRMSE for y_hat2 = 2*x + 0.5: tensor(0.3955)\n\n\nThe second line has a lower loss, and is therefore a better fit. Tweak the parameters and see if you can do even better.\nTHINK: What is your thought process as you try this? Are you doing something like the gradient descent described earlier?\n\n\n2.4 Calculating Gradients\nSo, how do we find the gradients we keep talking about? If you’re good at differentiation, you can look at a function and figure them out analytically. But this quickly breaks down when the function is complex or involves many steps. Fortunately, PyTorch does something called automatic differentiation, where it can keep track of every operation that happens to a tensor. It builds something called a computational graph, and when you want to calculate the gradients with respect to some final result you can simply call .backward() and PyTorch will trace the path back through this graph filling in the gradients at each step. We won’t go too deep into this, but here’s a simple example:\n\n# Some operations to demonstrate autograd\na = torch.tensor(5.7)\na.requires_grad = True # This is important - by default PyTorch won't track gradients\n\nb = 5*a + 2\n\nc = torch.sin(2*b+0.1)\n\nc\n\ntensor(-0.9871, grad_fn=<SinBackward0>)\n\n\nNotice that grad_fn bit there? Because c depends on something (b) that depends on a tensor that requires_grad (a), PyTorch keeps track of the function needed to calculate the gradients. We could then see the gradient of c with respect to a with:\n\nc.backward() # Tell pytorch to propagate the gradients backwards down the chain of operations\na.grad # See the resulting gradient\n\ntensor(-1.6036)\n\n\nThe derivative of c with respect to a is (10*cos(10*(a+0.41)) - plugging in a=5.7 we see that this does indeed give the answer dc/da = -1.603. This is quite magical - we can chain complex functions together and as long as eveything is differentiable we can rely on PyTorch to be able to work backwards and give us all the gradients we need.\n\n\nGradient Descent on our Toy Example\nLet’s get back to that example we were playing with, trying to find the parameters for a line that best describes the trend in our data.\nWe create our parameters w (initialized to 2 random floats) and tell pytorch to keep track of gradients.\nThen, in a loop, we repeatedly find the loss, find the gradients (loss.backward()) and update the parameters accordingly. We could do this ourselves but PyTorch provides an optimizer that handles the update for us - torch.optim.SGD. The learning rate lr determines how small of a step we take at each iteration.\nOnce the loop has finished running, we plot the losses and see that we are indeed getting better and better over time.\n\nw = torch.rand(2) # Our parameters\nw.requires_grad = True # Explain\n\noptimizer = torch.optim.SGD([w], lr=0.2) # Research: What does SGD stand for?\n\nlosses = [] # Keep track of our losses (RMSE values)\nws = [] # Keep track of the values we predicted\n\nfor i in range(100):\n\n    # Reset everything related to gradient calculations\n    optimizer.zero_grad()\n\n    # Get our outputs\n    y_hat = w[0]*x + w[1] \n\n    # Calculate our loss\n    loss = rmse(y, y_hat)\n\n    # Store the loss and a copy of the weights for later\n    losses.append(loss.detach().item())\n    ws.append(w.clone().detach().numpy())\n\n    # Print out updates ever few iterations\n    if i % 20 == 0:\n        print('loss at step', i, ':', loss)\n\n    # Backpropagate the loss and use it to update the parameters\n    loss.backward() # This does all the gradient calculations\n    optimizer.step() # The optimizer does the update. \n\n\nplt.plot(losses)\nplt.title('Loss over time')\n\nloss at step 0 : tensor(1.0562, grad_fn=<PowBackward0>)\nloss at step 20 : tensor(0.5021, grad_fn=<PowBackward0>)\nloss at step 40 : tensor(0.3565, grad_fn=<PowBackward0>)\nloss at step 60 : tensor(0.2702, grad_fn=<PowBackward0>)\nloss at step 80 : tensor(0.2418, grad_fn=<PowBackward0>)\n\n\nText(0.5, 1.0, 'Loss over time')\n\n\n\n\n\nOur random parameters have been updated 100 times and are now close to as good as they can possibly get:\n\nw # View the learned parameters\n\ntensor([3.0881, 0.0810], requires_grad=True)\n\n\n\nw.grad # We can see the gradients of the loss with respect to w (now small since we're close to optimum)\n\ntensor([-0.0208,  0.0105])\n\n\n\n# Plot predictions with these parameters\nplt.scatter(x, y, label='training data')\ny_hat = w[0]*x + w[1]\nplt.plot(x, y_hat.detach(), c='red', label='y_hat (with learned parameters)')\nplt.legend();\n\n\n\n\nSince we only have two parameters, we can make a plot that shows the loss for every combination of values within some range. We’ll plot the values of the parameters during the optimization loop above as points, and you can see how they slowly ‘move’ towards a point with lower loss:\nInstead of optimizer.step(), we could do w -= w.grad * 0.2 where 0.2 is the learning rate and the minus sign is because we want to move in the direction that reduces loss (so opposite to the steepest gradient).\nEXERCISE:: Try this and confirm for yourself that this works. (You’ll need with torch.no_grad(): w -= w.grad * 0.2 or PyTorch will complain - try it without first for a glimpse at an error you’ll likely meet a few more times in life ;)\nTHINK: Does this make sense? Are there any issues? What happens when the gradients are small? What happens when our step size (learning rate) is too high?\nTHINK: What kinds of problems can we solve with this tool? Can you think of examples? What are the limitations?"
  },
  {
    "objectID": "optimization.html#optimization-methods",
    "href": "optimization.html#optimization-methods",
    "title": "Lesson 2: Gradient Descent and Optimization",
    "section": "Optimization Methods",
    "text": "Optimization Methods\nGradient Descent (or Stochastic Gradient Descent, which is just GD on batches of data rather than the full dataset) is just one optimization method. There are many improvements that can be made. If you’re interested, here is a great rundown of the many alternatives that are used today: https://ruder.io/optimizing-gradient-descent/\nOne useful idea that is bundled with optimizers in PyTorch is that of regularization. It’s a large topic, but in essence regularization is concerned with smoothing things out and simplifying models or parameter sets by avoiding any values that are too extreme.\nAt some point I hope to add another notebook for exploring this, for now just remember that there are lots of different choices of optimizer available in PyTorch and they can be fun to experiment with and compare."
  },
  {
    "objectID": "optimization.html#demo-time",
    "href": "optimization.html#demo-time",
    "title": "Lesson 2: Gradient Descent and Optimization",
    "section": "Demo Time!",
    "text": "Demo Time!\nWe’ve solved the toy problem above with optimization. What else can we do with this? Well, for one thing this same approach is used to train pretty much all neural networks in use today! We’ll look at how that works in a future lesson. For now, you might enjoy checking out the bonus notebook ‘Fun With Generators and Losses’ where we look at a number of ‘generators’ (functions with parameters we can optimize that produce and image) and a number of ‘losses’ which we can use to get different effects. For example, here’s how we’d optimize a network based on sinusoidal functions to match a text description (leaning on some magical algorithms like CLIP which we haven’t covered yet):\n\nfrom tglcourse.generation_utils import SirenGenerator, CLIPLossToTargets, optimise\nimport torch\n\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ngen = SirenGenerator().to(device)\nclip_loss_fn = CLIPLossToTargets(text_prompts=['A painting of a jellyfish by a coral reef'], n_cuts=32, device=device)\noptimizer = torch.optim.Adam(gen.parameters(), lr=0.01)\noptimise(gen, [clip_loss_fn], optimizer=optimizer, n_steps=50)\n\nIf you dig into the code for that optimise function you’ll see that it is essentially just the same as the optimization loop we made earlier in this notebook!"
  },
  {
    "objectID": "optimization.html#where-next",
    "href": "optimization.html#where-next",
    "title": "Lesson 2: Gradient Descent and Optimization",
    "section": "Where next",
    "text": "Where next\nThe bonus notebook ‘Fun with Generators and Losses’ and the accompanying video are fairly complimentary to this notebook. The video especially shows a very high-level view of optimization and how this single idea underlies a TON of deep learning applications. Lots of the code in the notebook will look unfamiliar since we haven’t covered many of the building blocks in this course yet - give it a skim now and then re-read it again after, say, lesson 5.\n\n\n\n\n\nTwo other ideas for how to keep busy until the next lesson:  - Check out The spelled-out intro to neural networks and backpropagation: building micrograd where Andrej Karpathy trains a neural network from scratch, including implementing the gradient calculations and things which we’ve been offloading to PyTorch. Great if you like to see things built from the bottom up! - Go back through this notebook and try to tweak as many things as you can. Re-implement the loss calculation without peeking at the RMSE function, change the starting values of the parameters we’re optimizing, try a different optimizer (see the torch documentation) or play with changing the learning rate. Can you find a way to get a good solution with fewer steps? Does it ever go wrong and give crazy results or errors? - Look for code online for training a neural network in PyTorch. Can you spot the optimization loop? More generally, can you find examples online of deep learning tasks and identify the three ingredients shown above in each case?"
  },
  {
    "objectID": "optimization.html#a-dose-of-ethics-objectives-matter",
    "href": "optimization.html#a-dose-of-ethics-objectives-matter",
    "title": "Lesson 2: Gradient Descent and Optimization",
    "section": "A Dose Of Ethics: Objectives Matter",
    "text": "A Dose Of Ethics: Objectives Matter\nLet’s say you want to predict recidivism rates (how likely a person is to commit a crime once released). What should your loss function be? Perhaps you can look at past cases, see who was re-arrested and train a model to maximise accuracy when predicting this. After all, arrests are a good way to measure crimes, right? But what if some neighbourhoods have more police presence, or the officers are more likely to arrest some subset of the population? If the measure is biased, the algorithm will be biased, and a biased algorithm can mean unfair outcomes. This is a big problem for risk prediction software and is one example of the kinds of issues that can arise when you pick the wrong metric to optimize.\nFrom content recommendation systems pushing outrage-inducing clickbait to maximise ‘engagement’, to chatbots spewing hate-speech, failures to consider the consequences of optimizing towards a single metric abound. We’re going to have a lot of fun with optimization, but remember: with great fun… comes great responsibility.\nPage stats: Total Hits:  Page visitors:"
  },
  {
    "objectID": "pytorch_basics.html",
    "href": "pytorch_basics.html",
    "title": "Lesson 1: PyTorch Basics",
    "section": "",
    "text": "PyTorch is primarily a deep learning framework. It has been designed to make creating and working with deep neural networks as easy, fast and flexible as possible. Today we’ll look at one of the core components that makes this possible: tensors. We’ll start by looking at how to contruct and manipulate tensors, and then apply some of these ideas by representing images as tensors and seeing what we can do with that.\nRemember: this isn’t an exhaustive reference! The goal here is just to begin building a bit of familiarity with some tensor operations - you can always come back later or dig into the excellent PyTorch Documentation for a more complete explanation of what all of these functions can do!\nAs mentioned in the intro video above, you’re welcome to reach out via Discord if you have questions. There is a long-form notebook run-through video here where I go through all of the code and explain things in a bit more detail."
  },
  {
    "objectID": "pytorch_basics.html#creating-tensors",
    "href": "pytorch_basics.html#creating-tensors",
    "title": "Lesson 1: PyTorch Basics",
    "section": "Creating Tensors",
    "text": "Creating Tensors\nWe can construct a tensor directly from some common python iterables, such as list and tuple. Nested iterables can also be handled as long as the dimensions make sense.\n\n# tensor from a list\na = torch.tensor([0, 1, 2])\nprint(f\"Tensor a: {a}\")\n\n#tensor from a tuple of tuples\nb = ((1.0, 1.1), (1.2, 1.3))\nb = torch.tensor(b)\nprint(f\"Tensor b: {b}\")\n\n# tensor from a numpy array\nc = np.ones([2, 3])\nc = torch.tensor(c)\nprint(f\"Tensor c: {c}\")\n\nTensor a: tensor([0, 1, 2])\nTensor b: tensor([[1.0000, 1.1000],\n        [1.2000, 1.3000]])\nTensor c: tensor([[1., 1., 1.],\n        [1., 1., 1.]], dtype=torch.float64)\n\n\nIt’s easy to get confused as to what ‘shape’ a tensor has. So debugging tip #1: when in doubt, print out the shape!\n\nprint(b.shape)\n\ntorch.Size([2, 2])\n\n\nThere are also various constructor methods you can use. The arguments determine the size - explore changing these are see what happens to the output:\n\nx = torch.ones(5, 3)\ny = torch.zeros(2)\nz = torch.empty(1, 1, 5)\nprint(f\"Tensor x: {x}\")\nprint(f\"Tensor y: {y}\")\nprint(f\"Tensor z: {z}\")\n\nTensor x: tensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]])\nTensor y: tensor([0., 0.])\nTensor z: tensor([[[-1.6948e+14,  4.5755e-41, -6.6485e-26, -1.6412e+22, -1.7098e+14]]])\n\n\nNotice that .empty() does not return zeros, but seemingly random small numbers. Unlike .zeros(), which initialises the elements of the tensor with zeros, .empty() just allocates the memory. It is hence a little bit faster if you are looking to just create a tensor.\nThere are also constructors for random numbers:\n\n# uniform distribution\na = torch.rand(1, 3)\n\n# normal distribution\nb = torch.randn(3, 4)\n\nprint(f\"Tensor a: {a}\")\nprint(f\"Tensor b: {b}\")\n\nTensor a: tensor([[0.9829, 0.4177, 0.3677]])\nTensor b: tensor([[ 1.0115,  0.2457, -0.6459, -1.9705],\n        [-0.1782,  0.4208, -0.1519,  0.2699],\n        [-0.2137, -2.2777, -0.5582, -2.0515]])\n\n\nTHINK: What’s the difference? If you’re curious, use plt.hist(torch.randn(100)) to view the distribution.\nThere are also constructors that allow us to construct a tensor according to the above constructors, but with dimensions equal to another tensor:\n\nc = torch.zeros_like(a)\nd = torch.rand_like(c)\nprint(f\"Tensor c: {c}\")\nprint(f\"Tensor d: {d}\")\n\nTensor c: tensor([[0., 0., 0.]])\nTensor d: tensor([[0.6201, 0.9000, 0.1410]])\n\n\nFinally, .arange() and .linspace() behave how you would expect them to if you are familar with numpy.\n\na = torch.arange(0, 10, step=1) # Equivalent to np.arange(0, 10, step=1)\nb = torch.linspace(0, 5, steps=11) # np.linspace(0, 5, num=11)\n\nprint(f\"Tensor a: {a}\\n\")\nprint(f\"Tensor b: {b}\\n\")\n\nTensor a: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\nTensor b: tensor([0.0000, 0.5000, 1.0000, 1.5000, 2.0000, 2.5000, 3.0000, 3.5000, 4.0000,\n        4.5000, 5.0000])\n\n\n\nAgain, play with the code above to get familiar with the different arguments and how they affect the output. If you’re not sure of what arguments a function takes, you can query it from Jupyter. Remove the # then run this cell to see the docstring.\n\n#?torch.linspace"
  },
  {
    "objectID": "pytorch_basics.html#tensor-operations",
    "href": "pytorch_basics.html#tensor-operations",
    "title": "Lesson 1: PyTorch Basics",
    "section": "Tensor Operations",
    "text": "Tensor Operations\nWe can perform operations on tensors using methods under torch.. However, in PyTorch most common Python operators are overridden, so we can use those instead. The common standard arithmetic operators (+, -, *, /, and **) have all been lifted to elementwise operations.\n\nx = torch.tensor([1, 2, 4, 8])\ny = torch.tensor([1, 2, 3, 4])\nprint('Addition via torch.add:', torch.add(x, y))\nprint('Addition using \"+\":', x+y) # The same\nprint('Some other operations:')\nx + y, x - y, x * y, x / y, x**y  # The ** operator is exponentiation\n\nAddition via torch.add: tensor([ 2,  4,  7, 12])\nAddition using \"+\": tensor([ 2,  4,  7, 12])\nSome other operations:\n\n\n(tensor([ 2,  4,  7, 12]),\n tensor([0, 0, 1, 4]),\n tensor([ 1,  4, 12, 32]),\n tensor([1.0000, 1.0000, 1.3333, 2.0000]),\n tensor([   1,    4,   64, 4096]))\n\n\nTHINK: What does ‘element-wise’ mean? Inspect the outputs above.\nTensors also have many built-in methods such as .mean() or .sum() (see the full list here: https://pytorch.org/docs/stable/tensors.html). Whenever you’re working with a multi-dimensional tensor, pay attention to the dimensions and think about what result you’re aiming to achieve.\n\nx = torch.rand(3, 3)\nprint('x:\\n', x)\nprint(f\"Sum of every element of x: {x.sum()}\")\nprint(f\"Sum of the columns of x: {x.sum(axis=0)}\")\nprint(f\"Sum of the rows of x: {x.sum(axis=1)}\")\n\nx:\n tensor([[0.4153, 0.6779, 0.5828],\n        [0.5472, 0.5127, 0.6336],\n        [0.3987, 0.8190, 0.7021]])\nSum of every element of x: 5.2892351150512695\nSum of the columns of x: tensor([1.3612, 2.0096, 1.9185])\nSum of the rows of x: tensor([1.6760, 1.6934, 1.9198])\n\n\nRemember we said most operations default to ‘element-wise’? What if we want the matrix operation? Torch has you covered there as well. torch.matmul() or the @ symbol let you do matrix multiplication. For dot multiplication, you can use torch.dot().\nTransposes of 2D tensors are obtained using torch.t() or Tensor.T. Note the lack of brackets for Tensor.T - it is an attribute, not a method.\n\na = torch.rand(2, 3)\nb = a.T\nprint('a.shape:', a.shape, 'b.shape:', b.shape)\nprint(a@b) # Matrix multiplication of a 2x3 with a 3x2 matrix gives a 2x2 result.\n\na.shape: torch.Size([2, 3]) b.shape: torch.Size([3, 2])\ntensor([[0.5995, 0.9596],\n        [0.9596, 1.9312]])"
  },
  {
    "objectID": "pytorch_basics.html#manipulating-tensors",
    "href": "pytorch_basics.html#manipulating-tensors",
    "title": "Lesson 1: PyTorch Basics",
    "section": "Manipulating Tensors",
    "text": "Manipulating Tensors\nBeyond mathematical operations, we often want to access specific items or sets if items in a tensor, or perform operations like changing the shape of a tensor. Here are a few examples of some common tasks. These may feel simple if you’re used to something like numpy, but it’s worth making sure you know how to do these basic operations (or at least, you know where to find these examples again to refer to them!) since we’ll use these a lot in the coming lessons.\n\n# Indexing tensors\nx = torch.arange(0, 10)\nprint(x)\nprint(x[-1])\nprint(x[1:3]) # From index 1 up to but NOT INCLUDING index 3\nprint(x[:-2])\n\ntensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\ntensor(9)\ntensor([1, 2])\ntensor([0, 1, 2, 3, 4, 5, 6, 7])\n\n\nReshaping works as long as the shapes make sense. (3, 4) -> (4, 3) is fine, but (3, 4) -> (8, 2) won’t work since there aren’t enough elements!\n\nprint('Starting tensor:')\nz = torch.arange(1, 13)\nprint('z.shape:', z.shape)\nprint(f'z: {z}\\n')\n\nprint('Reshaping to (3, 4):')\nz = z.reshape(3, 4)\nprint('z.shape:', z.shape)\nprint(f'z:{z}\\n')\n\nprint('Flattening:')\nz = z.flatten()\nprint('z.shape:', z.shape)\nprint(f'z: {z}')\n\nStarting tensor:\nz.shape: torch.Size([12])\nz: tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])\n\nReshaping to (3, 4):\nz.shape: torch.Size([3, 4])\nz:tensor([[ 1,  2,  3,  4],\n        [ 5,  6,  7,  8],\n        [ 9, 10, 11, 12]])\n\nFlattening:\nz.shape: torch.Size([12])\nz: tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])\n\n\nConcatenating tensors is done with torch.cat - take a look at this examples and take note of how the dimension specified affects the output:\n\n# Create two tensors of the same shape\nx = torch.arange(12, dtype=torch.float32).reshape((3, 4))\ny = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n\n\n#concatenate them along rows\ncat_rows = torch.cat((x, y), dim=0)\n\n# concatenate along columns\ncat_cols = torch.cat((x, y), dim=1)\n\n# printing outputs\nprint('Concatenated by rows: shape{} \\n {}'.format(list(cat_rows.shape), cat_rows))\nprint('\\n Concatenated by colums: shape{}  \\n {}'.format(list(cat_cols.shape), cat_cols))\n\nConcatenated by rows: shape[6, 4] \n tensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [ 2.,  1.,  4.,  3.],\n        [ 1.,  2.,  3.,  4.],\n        [ 4.,  3.,  2.,  1.]])\n\n Concatenated by colums: shape[3, 8]  \n tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])"
  },
  {
    "objectID": "pytorch_basics.html#squeezing-tensors",
    "href": "pytorch_basics.html#squeezing-tensors",
    "title": "Lesson 1: PyTorch Basics",
    "section": "Squeezing Tensors",
    "text": "Squeezing Tensors\nWhen processing batches of data, you will quite often be left with singleton dimensions. e.g. [1,10] or [256, 1, 3]. This dimension can quite easily mess up your matrix operations if you don’t plan on it being there…\nIn order to compress tensors along their singleton dimensions we can use the .squeeze() method. We can use the .unsqueeze() method to do the opposite.\n\nx = torch.randn(1, 10)\nprint(x.shape)\nprint(f\"x[0]: {x[0]}\") # printing the zeroth element of the tensor will not give us the first number!\n\ntorch.Size([1, 10])\nx[0]: tensor([-0.0246, -0.1939, -1.8423, -1.0410, -0.7128,  1.0612,  0.3478,  0.7456,\n        -0.7619, -1.0841])\n\n\n\n# lets get rid of that singleton dimension and see what happens now\nx = x.squeeze(0)\nprint(x.shape)\nprint(f\"x[0]: {x[0]}\")\n\ntorch.Size([10])\nx[0]: -0.024552516639232635\n\n\n\ny = torch.randn(5, 5)\nprint(f\"shape of y: {y.shape}\")\n\n# lets insert a singleton dimension\ny = y.unsqueeze(1) # Note the argument here is 1 - try 0 and 2 and make sure you get a feel for what unsqueeze does. \nprint(f\"shape of y: {y.shape}\")\n\nshape of y: torch.Size([5, 5])\nshape of y: torch.Size([5, 1, 5])"
  },
  {
    "objectID": "pytorch_basics.html#images-as-tensors",
    "href": "pytorch_basics.html#images-as-tensors",
    "title": "Lesson 1: PyTorch Basics",
    "section": "Images as Tensors",
    "text": "Images as Tensors\n\n\n\n        \n        \n\n\nNow that we know how to work with tensors, let’s take a look at how we might represent and manipulate images as tensors.\n\nImages as arrays of numbers\n\nim = Image.open('images/frog.png').convert('RGB').resize((128, 128))\nim\n\n\n\n\n\nnp.array(im).shape # Remove .convert('RGB') in the cell above and see how this changes\n\n(128, 128, 3)\n\n\nWe’ll want to do this a bunch during the course, so let’s create a function to load an image using PIL and export it so that we can later access it in other notebooks:\n\nsource\n\n\nload_image_pil\n\n load_image_pil (fn, size=None)\n\nAnd to test that it works:\n\nload_image_pil('images/frog.png', size=(32, 32))\n\n\n\n\nIt will probably come in handy to be able to load images from a url, so let’s export a function for that too:\n\nsource\n\n\npil_from_url\n\n pil_from_url (url, size=None)\n\n\npil_from_url(\"https://images.pexels.com/photos/156934/pexels-photo-156934.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1\", size=(250, 200))\n\n\n\n\n\n\nConverting to Tensors\nWe can get a numpy array from a PIL image like so:\n\nim = load_image_pil('images/frog.png', size=(32, 32))\nimage_array = np.array(im)\n\n\nimage_array.shape\n\n(32, 32, 3)\n\n\nThis array has shape (32, 32, 3). The (3) is the channels (Red, Green, Blue). The values are integers between 0 and 255 (8-bit numbers). For compatability with various tools in the PyTorch world, we’re going to shuffle things around so that the channels dimension comes first, and we’ll represent images as floats between 0 and 1:\n\ntensor_im = torch.tensor(np.array(im)).permute(2, 0, 1)/255.0\ntensor_im.shape\n\ntorch.Size([3, 32, 32])\n\n\n\ntensor_im.min(), tensor_im.max()\n\n(tensor(0.), tensor(0.7804))\n\n\nThe torchvision library does this for us too:\n\nto_tensor_transform = torchvision.transforms.ToTensor()\nto_tensor_transform(im).shape\n\ntorch.Size([3, 32, 32])\n\n\nThis channels-first representation is popular but not universal - keep an eye out for variants if you’re using other people’s code :) \nThe final step here is that pytorch also typically works on batches of data, and so for images we usually have a tensor of shape (batch_size, channels, width, height). \nSince this back-and-forth translation is going to be something we do continuously during this course, let’s write functions to do this for us and export them so that we can use these in all the other notebooks:\n\nsource\n\n\ntensor_to_pil\n\n tensor_to_pil (tensor_im)\n\n\nsource\n\n\npil_to_tensor\n\n pil_to_tensor (im)\n\nLet’s see this in action:\n\nim = load_image_pil('images/frog.png', size=(128, 128)) # Load an image\ntensor_im = pil_to_tensor(im) # To tensor with our function\nprint(tensor_im.shape)\nim_out = tensor_to_pil(tensor_im) # And back to a PIL imge\nprint(type(im_out))\nim_out\n\ntorch.Size([1, 3, 128, 128])\n<class 'PIL.Image.Image'>\n\n\n\n\n\nAnother thing we’ll do is visualize images or specific image channels using matplotlib. It is fine with images being floats between 0 and 1, but expects the channels dimension first (for RGB images) or no channel dimension (for single-channel images). To demo this here’s how we might plot the three separate color channels in an image:\n\n# Plotting the original image and the three color channels\nfig, axs = plt.subplots(1, 4, figsize=(16, 5))\naxs[0].imshow(tensor_im[0].permute(1, 2, 0)) # Note: we need to rearrange the color channels\naxs[0].set_title('Original image')\ncolors = ['red', 'green', 'blue']\nfor i in range(1, 4):\n    axs[i].imshow(tensor_im[0][i-1], cmap='gray')\n    axs[i].set_title(f'{colors[i-1]} channel')\n\n\n\n\nWe can index into the tensor image to access specific slices or parts of the image. For the plotting above, we accessed each channel independantly. We can also select all channels but only a subset of pixels:\n\ncropped = tensor_im[0, :, 50:100, 50:120] # First image in batch, all channels, 50px high and 70 wide starting from (50, 50) (top left is 0, 0)\ntensor_to_pil(cropped)\n\n\n\n\n\n# Exercise: Can you make a greyscale image by taking the mean of the three color channels? \n# You may need to look at the pytorch docs for 'mean', as well as 'expand' or 'repeat' or 'cat' to turn the single-channel \n# result back into a three-channel image.\n\n\n\nTransforms\nWe’ll often want to perform some kind of image transform, such as zooming, rotating, warping or cropping. You can do this with the Python Imaging Library (PIL) but for performance reasons it is also nice to be able to do these operations on tensors, which is where the torchvision library comes in. Torchvision provides a number fo transforms which we can use on images, including random transforms which are useful for data augmentation. Uncomment the cell below to see a list of some of the available options, and try a few out for yourself:\n\n# print(dir(torchvision.transforms)[:-16]) # Uncomment to see some of the available options\n\n\ntransform = torchvision.transforms.RandomAffine(30)\ntensor_to_pil(transform(tensor_im))\n\n\n\n\n\n\nExercise: Least Average Image\nLet’s end this lesson with a bit of an exercise. Given the tensor operations we’ve looked at, can you find an interesting way to combine a number of images into a single output? For example, given a set of images, for each pixel location find the furthest value from the mean. Inspiration: https://www.andreweckel.com/LeastAverageImage/  Experiment with variations on this idea. \nAn example with stills from a video I made:\n\n\n\n\n\nAnd one made in a slightly different way with 5 random images from this course:\n\n\n\n\n\n\n# Your code here..."
  },
  {
    "objectID": "pytorch_basics.html#a-dose-of-ethics-cameras-and-race",
    "href": "pytorch_basics.html#a-dose-of-ethics-cameras-and-race",
    "title": "Lesson 1: PyTorch Basics",
    "section": "A Dose of Ethics: Cameras and Race",
    "text": "A Dose of Ethics: Cameras and Race\nCameras often have a limit on their ‘dynamic range’ - how much difference there can be between the lightest and darkest region of an image. Some sensors can capture more information in the RAW file format, but then processing needs to be done to compress that range down to be represented as the 8-bit jpegs that the rest of us see. A lot of time the camera settings and processing pipeline are automated or use a set of defaults tuned to be convenient for the “average user”.\nWhat does that have to do with race or ethics? Anyone with dark skin will be able to tell you: the defaults often suck for anyone with a different skin tone! Slow progress is being made on adapting photography tools to better handle skin tone variation, but for many years the defaults have been tied to a specific subset of humanity, which in turn has had knock-on effects on who gets represented in image data, which in turn means that things like facial recognition algorithms perform measurably worse on some populations.\nAs you move through this course, keep this in mind: even seeimgly arbitrary technical choices (such as default exposure settings) can have consequences for people down the line. Let’s work towards designing inclusive systems that can benefit as many people as possible.\nPage stats: Total Hits:  Page visitors:"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Each lesson will have some small exercises for you to try out some of the ideas, but I’ve also designed three larger projects for you to show off your skills. At some point TBD I’d like to figure out getting certificates for those who complete these, but have yet to figure out the details. This page will host the project descriptions and info on submitting yours once that’s all figured out."
  },
  {
    "objectID": "projects.html#project-1-the-golden-generation",
    "href": "projects.html#project-1-the-golden-generation",
    "title": "Projects",
    "section": "Project 1: The Golden Generation",
    "text": "Project 1: The Golden Generation\nOnce you’re done with lessons 1-5, check out the bonus notebook ‘Fun with Generators and Losses’ [TODO link]. Your goal is to come up with a custom pipeline for generating images. A user should be able to provide minimal input (perhaps an input image, or a piece of text, or both) and get back something special. You can acheive this in a number of ways: - Pick an existing generator alongside one or more loss functions and set them up with good default settings - Create a new generator following the template in that notebook - Create your own custom loss function - Chain together a sequence of steps where the output of one step feeds into the next.\nPackage the resulting pipeline into a gradio interface (see the bonus notebook ‘TODO Gradio NB’ for a guide) and share it as a huggingface space.\n\n\n\nwaterface demo\n\n\nFor inspiration, check out my ‘waterface’ demo that first processes an input image to look like a sketch then optimizes an imstack of the result towards a CLIP prompt: https://huggingface.co/spaces/johnowhitaker/waterface"
  },
  {
    "objectID": "projects.html#project-2-we-gan-do-it",
    "href": "projects.html#project-2-we-gan-do-it",
    "title": "Projects",
    "section": "Project 2: WE GAN DO IT!",
    "text": "Project 2: WE GAN DO IT!\nTrain a GAN on your own data - synthetic or curated. Try different architectures and training tricks, until you get a result you’re happy with. Document your experiments and results with a nice report. See TODO W&B tutorial for a guide on logging experiments.\nTODO example report, sweeps example"
  },
  {
    "objectID": "projects.html#project-3-looking-finetuned",
    "href": "projects.html#project-3-looking-finetuned",
    "title": "Projects",
    "section": "Project 3: Looking Fine(tuned)",
    "text": "Project 3: Looking Fine(tuned)\nFine-tune a diffusion model of your choice using your own dataset. Share it with the world - set up a custom pipeline, share training details in a report, make the model weights available and maybe set up a cool colab notebook for people to try your new model, complete with fancy features like CLIP guidance or image-to-image."
  },
  {
    "objectID": "projects.html#final-project",
    "href": "projects.html#final-project",
    "title": "Projects",
    "section": "Final Project",
    "text": "Final Project\nMake something amazing! You can adapt one of the projects above or forge your own path off the beated track. Whatever you choose, the goal of this final project is to take it to the next level :)\nPage stats: Total Hits:  Page visitors:"
  },
  {
    "objectID": "library.html",
    "href": "library.html",
    "title": "The Library",
    "section": "",
    "text": "This course is written in Jupyter notebooks. If you view the raw notebooks rather than the rendered website, you’ll see lots of extra syntax sprinkled around - thigs like ‘#|hide’ or ‘#|export’ at the top of a code cell. These are NBDev directives. Many are for visual formatting, but some (like ‘#|export’) tell the processing system to export parts of the code into separate files where they form components of the tglcourse library. This means that any functions or classes defined in the notebooks and exported in this way can be used at a later date. In this notebook we’ll demonstrate some of these functions and their potential uses - if you’re curious about each you can always go back to the lesson where it is defined for more background info.\nIf you haven’t already, make sure you install the library and its requirements with pip install tglcourse\nThis is a WIP and needs some love.\n\nfrom tglcourse.utils import *\n\n\nim = pil_from_url('https://johnowhitaker.github.io/tglcourse/index_files/figure-html/cell-2-output-1.png')\nim.resize((128, 128))\n\n\n\n\n\n# TODO generation example (fix device requirements for OTStyleLossToTarget and co)\n\n\n# TODO get list of all the things exported and what notebooks they come from\n\n\nimport glob\nfor f in glob.glob('tglcourse/*.py'):\n    if f.split('/')[1][0]!= '_':\n        with open(f, 'r') as ff:\n            print(f)\n            lines = ff.readlines()[:5] \n            print('From:', lines[0].split('../')[1].strip())\n            print('__all__:', lines[3].split('=')[1].strip(), '\\n')\n\ntglcourse/data_utils.py\nFrom: 61_Datasets.ipynb.\n__all__: ['to_tensor', 'mnist_transform', 'get_mnist_dl'] \n\ntglcourse/generation_utils.py\nFrom: 62_Generators_and_Losses.ipynb.\n__all__: ['PixelGenerator', 'MSELossToTarget', 'ImStackGenerator', 'calc_vgg_features', 'ContentLossToTarget', \n\ntglcourse/lesson12.py\nFrom: 12_DM1.ipynb.\n__all__: ['BasicConvNet', 'PreNormResidual', 'FeedForward', 'MLPMixer', 'BasicUNet', 'NoiseConditionedUNet'] \n\ntglcourse/utils.py\nFrom: 01_PyTorch_Basics.ipynb.\n__all__: ['load_image_pil', 'pil_from_url', 'pil_to_tensor', 'tensor_to_pil']"
  },
  {
    "objectID": "getting_started.html",
    "href": "getting_started.html",
    "title": "Getting Started",
    "section": "",
    "text": "Generative models are cool [no citation needed].\nIt is important that this cool new tech be accessible. Part of that is open models and code, but that isn’t much use if no-one but a select few can understand how they work. The goal of this course is to get you up to speed with the latest advancements in generative modelling, to the point where you can understand and contribute to the work being done in this area."
  },
  {
    "objectID": "getting_started.html#how-to-navigate-this-course",
    "href": "getting_started.html#how-to-navigate-this-course",
    "title": "Getting Started",
    "section": "How To Navigate This Course",
    "text": "How To Navigate This Course\n\n\n\n\n\nThis course is split up into four main components:  - ‘Lessons’ has the core material that makes up the bulk of the course content. You can work through all of these in order, or skip through to topics that interest you.  - ‘Bonus Material’ covers additional topics that compliment the main lessons. Setting up cloud machines, or tracking experiments - things that will make your experience working with deep learning richer and more productive. This also includes some important extra topics such as ethics in generative modelling. - ‘Projects’ outlines some recommended exercises for you to do as you proceed through the course material. These will also be used to assess participation if we decide to figure out some sort of certification (TBD).  - ‘Discussions’ features podcast-style interviews with interesting people working in this field, from artists to deep learning researchers. \nThis whole course is written as notebooks, so you can open each lesson in colab using [TODO open in colab button] to start running the code, or see [TODO make setting up in paperspace] to see how you can run the course in the cloud.\nThe recommended way to work through the course is to start at Lesson 1. If you already have some PyTorch skills you’ll be able to skim the first few lessons quite rapidly, but I still recommend taking a look and trying the exercises at the end of each notebook. Specific bonus materials and projects will get referenced from within the lessons once you get to the stage where they are relevant. Of course, you are also welcome to ignore this route and skip around wherever your interest takes you."
  },
  {
    "objectID": "getting_started.html#the-library",
    "href": "getting_started.html#the-library",
    "title": "Getting Started",
    "section": "The Library",
    "text": "The Library\nAs we progress through the lessons we’ll begin to build a library of useful functions and classes that will come in handy for various generative modelling tasks. Thanks to the magic of NBDEV these are automatically exported into the tglcourse library, which you can use in your own projects. An overview is available here.\nInstalling is as simple as pip install tglcourse. This is also the easiest way to get all of the requirements set up if you’re trying to run the lesson notebooks locally."
  },
  {
    "objectID": "getting_started.html#live-lessons-discussion-groups",
    "href": "getting_started.html#live-lessons-discussion-groups",
    "title": "Getting Started",
    "section": "Live Lessons + Discussion Groups",
    "text": "Live Lessons + Discussion Groups\nThe course is designed to be explored at your own pace, but we will be running a sort of ‘study group’ over on Discord with weekly discussions around some of the lessons. Final details are still TBC, but I’ll probably do one or more live run-throughs of the lesson notebooks and then make those recordings available alongside the lesson notebooks for those who prefer a longer-form video to keep them company as they explore the code."
  },
  {
    "objectID": "getting_started.html#contributing",
    "href": "getting_started.html#contributing",
    "title": "Getting Started",
    "section": "Contributing",
    "text": "Contributing\nThis entire course/library is built using NBDev. Every page is a notebook, and the docs, library code and webpage are all created from these source notebooks. The hope is that these will stay up-to-date with continual improvements and bug-fixes added.\nIf you spot a bug or have a request for an improvement, you can open an issue on GitHub to let me know, or send a message via Discord.\nI’d also love to hear about any guests you think would be good to have on for the discussion series, or any extra topics you’d like to see covered.\nPage stats: Total Hits:  Page visitors:"
  },
  {
    "objectID": "dm2.html",
    "href": "dm2.html",
    "title": "Fine-Tuning, Guidance and Conditioning",
    "section": "",
    "text": "This is the second ‘unit’ of the HuggingFace diffusion models course.\nIn this lesson, you will learn how to use and adapt pre-trained diffusion models in new ways. You will also see how we can create diffusion models that take additional inputs as conditioning to control the generation process."
  },
  {
    "objectID": "dm2.html#get-started",
    "href": "dm2.html#get-started",
    "title": "Fine-Tuning, Guidance and Conditioning",
    "section": "Get Started",
    "text": "Get Started\nHere are the steps for this unit: - Read through the material below for an overview of the key ideas of this unit - Check out the Fine-tuning and Guidance notebook to fine-tune an existing diffusion model on a new dataset using the 🤗 Diffusers library and to modify the sampling procedure using guidance - Follow the example in the notebook to share a Gradio demo for your custom model - (Optional) Check out the Class-conditioned Diffusion Model Example notebook to see how we can add additional control to the generation process. - (Optional) Check out the video below for an informal run-through of the material in this unit."
  },
  {
    "objectID": "dm2.html#fine-tuning",
    "href": "dm2.html#fine-tuning",
    "title": "Fine-Tuning, Guidance and Conditioning",
    "section": "Fine-Tuning",
    "text": "Fine-Tuning\nAs you may have seen in Unit 1, training diffusion models from scratch can be time-consuming! Especially as we push to higher resolutions, the time and data required to train a model from scratch can become impractical. Fortunately, there is a solution: begin with a model that has already been trained! This way we start from a model that has already learned to denoise images of some kind, and the hope is that this provides a better starting point than beginning from a randomly initialized model.\n\n\n\nExample images generated with a model trained on LSUN Bedrooms and fine-tuned for 500 steps on WikiArt\n\n\nFine-tuning typically works best if the new data somewhat resembles the base model’s original training data (for example, beginning with a model trained on faces is probably a good idea if you’re trying to generate cartoon faces) but surprisingly the benefits persist even if the domain is changed quite drastically. The image above is generated from a model trained on the LSUN Bedrooms dataset and fine-tuned for 500 steps on the WikiArt dataset. The training script is included for reference alongside the notebooks for this unit."
  },
  {
    "objectID": "dm2.html#guidance",
    "href": "dm2.html#guidance",
    "title": "Fine-Tuning, Guidance and Conditioning",
    "section": "Guidance",
    "text": "Guidance\nUnconditional models don’t give much control over what is generated. We can train a conditional model (more on that in the next section) that takes additional inputs to help steer the generation process, but what if we already have a trained unconditional model we’d like to use? Enter guidance, a process by which the model predictions at each step in the generation process are evaluated against some guidance function and modified such that the final generated image is more to our liking.\n\n\n\nguidance example image\n\n\nThis guidance function can be almost anything, making this a powerful technique! In the notebook, we build up from a simple example (controlling the color, as illustrated in the example output above) to one utilizing a powerful pre-trained model called CLIP which lets us guide generation based on a text description."
  },
  {
    "objectID": "dm2.html#conditioning",
    "href": "dm2.html#conditioning",
    "title": "Fine-Tuning, Guidance and Conditioning",
    "section": "Conditioning",
    "text": "Conditioning\nGuidance is a great way to get some additional mileage from an unconditional diffusion model, but if we have additional information (such as a class label or an image caption) available during training then we can also feed this to the model for it to use as it makes its predictions. In doing so, we create a conditional model, which we can control at inference time by controlling what is fed in as conditioning. The notebook shows an example of a class-conditioned model which learns to generate images according to a class label.\n\n\n\nconditioning example\n\n\nThere are a number of ways to pass in this conditioning information, such as - Feeding it in as additional channels in the input to the UNet. This is often used when the conditioning information is the same shape as the image, such as a segmentation mask, a depth map or a blurry version of the image (in the case of a restoration/superresolution model). It does work for other types of conditioning too. For example, in the notebook, the class label is mapped to an embedding and then expanded to be the same width and height as the input image so that it can be fed in as additional channels. - Creating an embedding and then projecting it down to a size that matches the number of channels at the output of one or more internal layers of the UNet, and then adding it to those outputs. This is how the timestep conditioning is handled, for example. The output of each Resnet block has a projected timestep embedding added to it. This is useful when you have a vector such as a CLIP image embedding as your conditioning information. A notable example is the ‘Image Variations’ version of Stable Diffusion which does exactly this. - Adding cross-attention layers that can ‘attend’ to a sequence passed in as conditioning. This is most useful when the conditioning is in the form of some text - the text is mapped to a sequence of embeddings using a transformer model, and then cross-attention layers in the UNet are used to incorporate this information into the denoising path. We’ll see this in action in Unit 3 as we examine how Stable Diffusion handles text conditioning."
  },
  {
    "objectID": "dm2.html#hands-on-notebook",
    "href": "dm2.html#hands-on-notebook",
    "title": "Fine-Tuning, Guidance and Conditioning",
    "section": "Hands-On Notebook",
    "text": "Hands-On Notebook\n\n\n\nChapter\nColab\nKaggle\nGradient\nStudio Lab\n\n\n\n\nFine-tuning and Guidance\n\n\n\n\n\n\nClass-conditioned Diffusion Model Example\n\n\n\n\n\n\n\nAt this point, you know enough to get started with the accompanying notebooks! Open them in your platform of choice using the links above. Fine-tuning is quite computationally intensive, so if you’re using Kaggle or Google Colab make sure you set the runtime type to ‘GPU’ for the best results.\nThe bulk of the material is in Fine-tuning and Guidance, where we explore these two topics through worked examples. The notebook shows how you can fine-tune an existing model on new data, add guidance, and share the result as a Gradio demo. There is an accompanying script (finetune_model.py) that makes it easy to experiment with different fine-tuning settings, and [an example space that you can use as a template for sharing your own demo on 🤗 Spaces.\nIn the Class-conditioned Diffusion Model Example, we show a brief worked example of creating a diffusion model conditioned on class labels using the MNIST dataset. The focus is on demonstrating the core idea as simply as possible: by giving the model extra information about what it is supposed to be denoising, we can later control what kinds of images are generated at inference time."
  },
  {
    "objectID": "dm2.html#project-time",
    "href": "dm2.html#project-time",
    "title": "Fine-Tuning, Guidance and Conditioning",
    "section": "Project Time",
    "text": "Project Time\nFollowing the examples in the Fine-tuning and Guidance notebook, fine-tune your own model or pick an existing model and create a Gradio demo to showcase your new guidance skills. Don’t forget to share your demo on Discord, Twitter etc so we can admire your work!"
  },
  {
    "objectID": "dm2.html#some-additional-resources",
    "href": "dm2.html#some-additional-resources",
    "title": "Fine-Tuning, Guidance and Conditioning",
    "section": "Some Additional Resources",
    "text": "Some Additional Resources\nDenoising Diffusion Implicit Models - Introduced the DDIM sampling method (used by DDIMScheduler)\nGLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models - Introduced methods for conditioning diffusion models on text\neDiffi: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers - Shows how many different kinds of conditioning can be used together to give even more control over the kinds of samples generated\nFound more great resources? Let us know and we’ll add them to this list."
  },
  {
    "objectID": "representations.html#what-do-networks-learn",
    "href": "representations.html#what-do-networks-learn",
    "title": "Lesson 4: Learning Representations + Style Transfer",
    "section": "What Do Networks Learn",
    "text": "What Do Networks Learn\n\n\n\n\n\nIn this lesson we’re going to look at some uses of large pretrained neural networks, and try to understand why they work as well as they do. To keep things organised, the content has been split between this notebook and two bonus notebooks.\n\nExtracting Represenations from Pretrained Models\nVGG16 was a fairly popular CNN that improved upon the famous AlexNet architecture. It has since been supeceeded by more complex networks but it still works great for our purposes. The architecture is something like the following (source):\n\nWe can load it like so (check the imports section at the start of this notebook for the relevant imports):\n\nvgg = vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n\n\n# vgg # Uncomment to show the sumary\n\n\n# Random 'images'\nx = torch.randn(8, 3, 128, 128) \n\n# Normalize with the same stats used during model training\ndef normalize(x):\n    mean = torch.tensor([0.485, 0.456, 0.406])[:,None,None]\n    std = torch.tensor([0.229, 0.224, 0.225])[:,None,None]\n    x = (x-mean) / std\n    return x\nx = normalize(x)\n\n# Pass through the model\nvgg(x).shape\n\ntorch.Size([8, 1000])\n\n\n\ndef get_penultimate_features(x):\n    x = vgg.features(x)\n    x = vgg.avgpool(x)\n    x = torch.flatten(x, 1)\n    for l in vgg.classifier[:-3]:\n        x = l(x)\n    return x\n\nfeatures = get_penultimate_features(x)\nfeatures.shape\n\ntorch.Size([8, 4096])\n\n\nFor each image we now have a feature vector containing 4096 values.\nWhat can we do with these? Let’s use them for image search!\n\nfrog_image = pil_from_url(\"https://images.pexels.com/photos/70083/frog-macro-amphibian-green-70083.jpeg?auto=compress&cs=tinysrgb&w=1600\", size=(128, 128))\nfrog_image\n\n\n\n\n\n# Get features from the frog image we're searching with\nfrog_features = get_penultimate_features(normalize(pil_to_tensor(frog_image)))\n\n# Get featurs for all images in the images/ folder\nimage_files = glob.glob('images/*')\npil_images = [load_image_pil(im, size=(128, 128)) for im in image_files]\nimages = [pil_to_tensor(pil_image) for pil_image in pil_images]\nx = torch.cat(images, dim=0)\nimage_features = get_penultimate_features(normalize(x))\n\n# Calculate the similarities\ncosine_sim =  torch.nn.CosineSimilarity()\nsimilarities = cosine_sim(frog_features, image_features)\nprint('Similarities:', similarities)\nbest_match_idx = torch.argmax(similarities).item()\nprint('Best match:', image_files[best_match_idx])\npil_images[best_match_idx]\n\nSimilarities: tensor([0.3742, 0.4047, 0.3873, 0.6293, 0.5818, 0.5095, 0.3890, 0.5207, 0.4447,\n        0.5527, 0.6004, 0.5090, 0.6598], grad_fn=<SumBackward1>)\nBest match: images/frog.png\n\n\n\n\n\nSome variant of this is used for most reverse-image-search implementations, and can be an extremely useful way to quickly ‘search’ a database of images, especially if you pre-compute the features for each image as it is added. Choosing how the network used for feature extraction is trained can determine what kinds of features are used - for example, a face recognition network will have features useful for finding similar faces, and can be used to find pictures of a specific person in a dataset."
  },
  {
    "objectID": "representations.html#transfer-learning",
    "href": "representations.html#transfer-learning",
    "title": "Lesson 4: Learning Representations + Style Transfer",
    "section": "Transfer Learning",
    "text": "Transfer Learning\n\n\n\n\n\nAs we said in the introduction, these learned ‘features’ capture a lot of useful information. When training an image classification network (for example) it is often beneficial to begin with a pretrained network rather than starting from scratch for this reason. Look at the bonus notebook to see an example of this in action, implemented in two different ways.\nClassifying Imagenet images is a pretty good pretraining task, and models trained on Imagenet tend to transfer well to other tasks. But this is not the only game in town! For specific applications, finding a pre-training task with lots of relevant data can help provide a model that will be a better starting point for downstream tasks. One example: training a model on satellite imagery on the task of predicting night-time light emmisions results in a model that has learn features associated with buildings and economic activity, which in turn can be fine-tuned on the task of estimating financial wellbeing from satellite imagery, as in this great paper from 2016. More generally, something like CLIP (which we’ll see in lesson 5) tends to learn extremely useful representations as a result of beig trained to associate images with captions across a large dataset."
  },
  {
    "objectID": "representations.html#style-transfer",
    "href": "representations.html#style-transfer",
    "title": "Lesson 4: Learning Representations + Style Transfer",
    "section": "Style Transfer",
    "text": "Style Transfer\nRemember how we said that early layers in a network tend to learn simpler features like colour and texture, while later layers capture more complex shapes? We’re going to take advantage of this to do a little bit of magic. We’ll take two images, a content image and a style image. Then we’ll feed both through a network and record the activations at various layers. Finally, we will try to optimize the pixels of a new image (or anything that produces an image) such that: \n\nThe content features (from later layers of the network) roughly match those from the content image \nThe types of style features (from early layers) are similar to those from the style image \n\nComparing the content features is relatively simple - something like MSE or MAE will suffice. But for the style features, we don’t want features in the same places, we only care that the same types of feature appear. The bonus notebook has three different takes on ways to measure this, and I hope to add more explanation in a dedicated section soon.\n\nstyle_image = pil_from_url(\"https://i.pinimg.com/originals/c3/b4/38/c3b438401bab3e91b487cd30309224f7.gif\", size=(512, 512)).convert('RGB')\nstyle_image.resize((128, 128)) # Small for preview\n\n\n\n\n\ncontent_image = load_image_pil('images/frog.png', size=(512, 512))\ncontent_image.resize((128, 128)) # Small for preview\n\n\n\n\n\nfrom tglcourse.generation_utils import PixelGenerator, VincentStyleLossToTarget, ContentLossToTarget, optimise\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\nstyle_loss_fn = VincentStyleLossToTarget(pil_to_tensor(style_image).to(device), size=512, style_layers = [3, 6, 9, 11])\ncontent_loss_fn = ContentLossToTarget(pil_to_tensor(content_image).to(device))\n\n# The pixels we'll optimise\ngen = PixelGenerator(512, init_image=content_image).to(device)\n\n# The optimizer - feel free to try different ones here\nopt = torch.optim.AdamW(gen.parameters(), lr=0.05, weight_decay=1e-6)\n\noptimise(gen, loss_functions=[style_loss_fn, content_loss_fn], optimizer=opt, n_steps=60)\n\n\n\n\n\n\n\n\n\n\n\n\ntensor_to_pil(gen())\n\n\n\n\nPretty neat hey! More background in the lesson live-stream and the bonus notebook ‘Fun with Generators and Losses’."
  },
  {
    "objectID": "representations.html#a-dose-of-ethics-i-like-your-style",
    "href": "representations.html#a-dose-of-ethics-i-like-your-style",
    "title": "Lesson 4: Learning Representations + Style Transfer",
    "section": "A Dose of Ethics: I Like Your Style",
    "text": "A Dose of Ethics: I Like Your Style\nStyle transfer is extremely fun. It is tempting to grab some random pics from Google Image Search and start playing, but please pause for a second and imagine yourself in the shoes of the artist or photographer whose work you’re about to download and modify. Perhaps you’d be happy to see your work used creatively, perhaps you’d be outraged. The image might have a licence that specifically restricts re-use or modification, and by using it you might be ‘ripping off’ a characteristic composition or style from someone who’d prefer you didn’t! Better to avoid any potential pitfalls and stick with your own images or works that are explicity in the public domain (or at least licenced permissively) - especially if you’re considering making money from the results.\nPage stats: Total Hits:  Page visitors:"
  },
  {
    "objectID": "wrapup.html",
    "href": "wrapup.html",
    "title": "Lesson 16: The Next Generation",
    "section": "",
    "text": "Congratulations! You made it to the end!\nPage stats: Total Hits:  Page visitors:"
  },
  {
    "objectID": "clip.html",
    "href": "clip.html",
    "title": "Lesson 5: Exploring Multiple Modalities with CLIP",
    "section": "",
    "text": "CLIP: - concepts - embedding text and images - measuring similarity - Use as a loss (imstack demo) - Need for transformations - export some functions for later - CLOOB\nCLIP (Contrastive Language-Image Pre-training) is a method created by OpenAI for training models capable of aligning image and text representations. Images and text are drastically different modalities, but CLIP manages to map both to a shared space, allowing for all kinds of neat tricks.\n\n\n\nclip diagram\n\n\nDuring training, CLIP takes in image-caption pairs. The images are fed through an image encoder model (based on a resnet or ViT backbone) and transformed into an embedding vector (I in the diagram above). A second model (typically a transformer model of some sort) takes the text and transforms it into an embedding vector (T in the diagram).\nCrucially, both these embeddings are the same shape, allowing for direct comparison. Given a batch of image-caption pairs, CLIP tries to maximize the similarity of image and text embeddings that go together (the blue diagonal in the diagram) while minimizing the similarity between non-related pairs.\nIn this lesson we’re going to explore some of the use-cases for a model like this, using an open CLIP implementation called OpenCLIP. If you’d like to read some more background on OpenCLIP and see some benchmarks I ran with some of the pretrained models, check out this report I wrote on the subject."
  },
  {
    "objectID": "clip.html#loading-clip-models-with-openclip",
    "href": "clip.html#loading-clip-models-with-openclip",
    "title": "Lesson 5: Exploring Multiple Modalities with CLIP",
    "section": "Loading CLIP models with OpenCLIP",
    "text": "Loading CLIP models with OpenCLIP\nYou can use pretrained CLIP models in a few different ways. There is OpenAI’s github repository or the HuggingFace implementations, but I like the OpenCLIP project which includes both OpenAI versions and a number of new models trained in the open on public datasets like LAION.\nLet’s import the library and see what models are available:\n\nimport open_clip\n\nprint(open_clip.list_pretrained())\n\n[('RN50', 'openai'), ('RN50', 'yfcc15m'), ('RN50', 'cc12m'), ('RN50-quickgelu', 'openai'), ('RN50-quickgelu', 'yfcc15m'), ('RN50-quickgelu', 'cc12m'), ('RN101', 'openai'), ('RN101', 'yfcc15m'), ('RN101-quickgelu', 'openai'), ('RN101-quickgelu', 'yfcc15m'), ('RN50x4', 'openai'), ('RN50x16', 'openai'), ('RN50x64', 'openai'), ('ViT-B-32', 'openai'), ('ViT-B-32', 'laion400m_e31'), ('ViT-B-32', 'laion400m_e32'), ('ViT-B-32', 'laion2b_e16'), ('ViT-B-32', 'laion2b_s34b_b79k'), ('ViT-B-32-quickgelu', 'openai'), ('ViT-B-32-quickgelu', 'laion400m_e31'), ('ViT-B-32-quickgelu', 'laion400m_e32'), ('ViT-B-16', 'openai'), ('ViT-B-16', 'laion400m_e31'), ('ViT-B-16', 'laion400m_e32'), ('ViT-B-16-plus-240', 'laion400m_e31'), ('ViT-B-16-plus-240', 'laion400m_e32'), ('ViT-L-14', 'openai'), ('ViT-L-14', 'laion400m_e31'), ('ViT-L-14', 'laion400m_e32'), ('ViT-L-14', 'laion2b_s32b_b82k'), ('ViT-L-14-336', 'openai'), ('ViT-H-14', 'laion2b_s32b_b79k'), ('ViT-g-14', 'laion2b_s12b_b42k'), ('roberta-ViT-B-32', 'laion2b_s12b_b32k'), ('xlm-roberta-base-ViT-B-32', 'laion5b_s13b_b90k')]\n\n\nFollowing the demo code on GitHub, here’s how we load a model:\n\nmodel, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32-quickgelu', pretrained='laion400m_e32')\n\n100%|████████████████████████████████████████| 605M/605M [00:05<00:00, 109MiB/s]\n\n\n\nAside: Model Naming\nWhat does ‘ViT-B/32’ mean? This uses a Vision Transformer (ViT) as the image encoder, with an image patch size of 32 (larger patch size means fewer patches per image at a given size, and thus faster/smaller models) and the ‘Base’ size model. I believe the size order is ‘Base’ (B), Large (L), Huge (H) and ginormous (g) but don’t quote me on that! Generally the larger models with the smaller patch sizes will need more memory and compute in exchenge for better performance. Most models were trained on 224px input images but there are variants trained on larger image sizes - again they will be more computationally expensive to run. ‘RN50’ style names mean the vision encoder is based on a Resnet50 architecture instead of a vision transformer.\nGenerally I like to stick with the smaller models, but you can experiment with swapping in larger variants or using multiple models and combining the results."
  },
  {
    "objectID": "clip.html#preprocessing",
    "href": "clip.html#preprocessing",
    "title": "Lesson 5: Exploring Multiple Modalities with CLIP",
    "section": "Preprocessing",
    "text": "Preprocessing\nWe want to prepare our image inputs to match the data used for training. In most cases, this involves resizing to 224px square images and normalizing the image data:\n\npreprocess # View the preprocessing function loaded with the model above\n\nCompose(\n    Resize(size=224, interpolation=bicubic, max_size=None, antialias=None)\n    CenterCrop(size=(224, 224))\n    <function _convert_to_rgb>\n    ToTensor()\n    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n)"
  },
  {
    "objectID": "clip.html#image---features",
    "href": "clip.html#image---features",
    "title": "Lesson 5: Exploring Multiple Modalities with CLIP",
    "section": "Image -> Features",
    "text": "Image -> Features\nWe can load in an image and pre-process it with this function, and then use model.encode_image to turn it into a CLIP embedding:\n\ninput_image = pil_from_url('https://images.pexels.com/photos/185032/pexels-photo-185032.jpeg?').resize((600, 400))\ninput_image\n\n\n\n\n\nimage = preprocess(input_image).unsqueeze(0)\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    image_features = model.encode_image(image)\n    \nprint('image.shape:', image.shape, 'image_features.shape:', image_features.shape)\n\nimage.shape: torch.Size([1, 3, 224, 224]) image_features.shape: torch.Size([1, 512])"
  },
  {
    "objectID": "clip.html#text---features",
    "href": "clip.html#text---features",
    "title": "Lesson 5: Exploring Multiple Modalities with CLIP",
    "section": "Text -> Features",
    "text": "Text -> Features\nFor text inputs, we first tokenize them (something you’ll learn more about in lesson 9) and then encode them with the encode_text method. Note that the resulting embeddings have the same number of dimensions as the image embedding above (512 in this case):\n\ntext = open_clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"])\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    text_features = model.encode_text(text)\n    \nprint('Tokenized text shape:', text.shape)\nprint('text_features.shape', text_features.shape)\n\nTokenized text shape: torch.Size([3, 77])\ntext_features.shape torch.Size([3, 512])"
  },
  {
    "objectID": "clip.html#computing-similarity",
    "href": "clip.html#computing-similarity",
    "title": "Lesson 5: Exploring Multiple Modalities with CLIP",
    "section": "Computing Similarity",
    "text": "Computing Similarity\nOnce we have representations of both images and text in the same space, we can compare them! Here’s how we might compare the three texts above with the seal image:\n\n# Computing similarities\ndef probability_scores(image_features, text_features):\n    image_features /= image_features.norm(dim=-1, keepdim=True)\n    text_features /= text_features.norm(dim=-1, keepdim=True)\n    text_probs = (image_features @ text_features.T).softmax(dim=-1)\n    return text_probs\n\ntorch.set_printoptions(precision=3)\nprint(\"Label probs:\", probability_scores(image_features, text_features))  # Apparently the closest guess is 'a dog' - do you agree?\n\nLabel probs: tensor([[0.330, 0.342, 0.328]])\n\n\nApparently the seal looks like a dog (the middle probability is higher). What is going on here?\nThis operation is very similar to the Cosine Similarity we saw in lesson 4. Specifically, the following two calulations are identical:\n\n(image_features/image_features.norm(dim=-1, keepdim=True))@(text_features/text_features.norm(dim=-1, keepdim=True)).T\n\ntensor([[0.143, 0.180, 0.138]])\n\n\n\ntorch.nn.functional.cosine_similarity(image_features, text_features)\n\ntensor([0.143, 0.180, 0.138])\n\n\nSo we’re calculating the cosine similarity between the image and each text, giving the values above. The final step in the classification demo is to run this through a softmax operation which will scale the values so that they sum to 1, allowing us to interpret them as probabilities:\n\ntorch.nn.functional.cosine_similarity(image_features, text_features).softmax(dim=-1)\n\ntensor([0.330, 0.342, 0.328])"
  },
  {
    "objectID": "clip.html#image-search",
    "href": "clip.html#image-search",
    "title": "Lesson 5: Exploring Multiple Modalities with CLIP",
    "section": "Image Search",
    "text": "Image Search\nWe can take advantage of this similarity measure for image search too! If you want to see an impressive large-scale version in action, check out the CLIP retrieval tool which lets you search LAION. For our demo let’s just search the images folder of this course:\n\n# Finding image files\nimport glob\nimage_files = glob.glob('images/*.png') + glob.glob('images/*.jpeg')\nlen(image_files)\n\n12\n\n\n\n# Claculating their CLIP embeddings\nimage_features = []\nfor fn in image_files:\n    im = load_image_pil(fn)\n    image_features.append(model.encode_image(preprocess(im).unsqueeze(0)))\nimage_features = torch.cat(image_features)\nimage_features.shape\n\ntorch.Size([12, 512])\n\n\n\n# Embedding a query text and finding the image with the highest similarity\ntext = open_clip.tokenize([\"a frog\"])\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    text_features = model.encode_text(text)\nsimilarities = torch.nn.functional.cosine_similarity(text_features, image_features)\nprint('Similarities:', similarities.detach().numpy())\nprint('Argmax:', similarities.argmax()) # TODO explain\nprint('Best image match:')\nload_image_pil(image_files[similarities.argmax()]).resize((128, 128))\n\nSimilarities: [0.12317517 0.07264645 0.07863379 0.11897033 0.0889878  0.07535253\n 0.07106286 0.11585622 0.12133323 0.12665415 0.27785698 0.13541897]\nArgmax: tensor(10)\nBest image match:\n\n\n\n\n\nPretty nifty! Now, what if instead of searching images we want to GENERATE them?"
  },
  {
    "objectID": "clip.html#using-clip-as-a-loss-function",
    "href": "clip.html#using-clip-as-a-loss-function",
    "title": "Lesson 5: Exploring Multiple Modalities with CLIP",
    "section": "Using CLIP as a loss function",
    "text": "Using CLIP as a loss function\nLet’s try optimizing the pixels of an image to mazimise the similarity of the image with a text prompt:\n\ndevice = 'cuda'\nmodel.to(device)\n\ntext = open_clip.tokenize([\"a picture of a frog\"]).to(device)\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    text_features = model.encode_text(text)\n    \nim = torch.rand(1, 3, 224, 224).to(device)\nstart_im = im.clone() # Keep a copy of the initial noise image\nim.requires_grad = True\n\nnormalize = torchvision.transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n\nopt = torch.optim.Adam([im], lr=1e-2)\nlosses = []\n\nfor i in tqdm(range(250)):\n    image_features = model.encode_image(normalize(im))\n    sim = torch.nn.functional.cosine_similarity(text_features, image_features)\n    loss = 1-sim # So lower is better\n    loss.backward()\n    losses.append(loss.item())\n    opt.step()\n    opt.zero_grad()\n    \ntensor_to_pil(im)\n\n\n\n\n\n\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(losses)\n\n\n\n\n\ntensor_to_pil(torch.abs(start_im - im)*2) # Viewing the difference between the start image and the final result\n\n\n\n\nWoah - what’s going on here?\n\nSome values in our image are > 1, which is not ideal\nThe CLIP model we’re using here is absed on a Vision Trnasformer, which works with image patches. You can see the patches in the difference image above - since each patch is seeing the same pixels all the time, those pixels can go crazy getting just the right output for that patch.\nThat learning rate might be a little high - you can try it lower or play with optmiizing for more iterations.\n\nSo, how do we fix all of this? Here’s a few improvements we can make:\nImprovements: - Changing our loss to something called the Great Circle Distance Squared - Applying some transforms to the image before feeding it to CLIP, such that the model sees a slightly different version of the image each time - Forcing the values to lie in the expected range for an image with im.clip(0, 1).\n\ntext = open_clip.tokenize([\"a picture of a frog\"]).to(device)\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    text_features = model.encode_text(text)\n    \nim = torch.rand(1, 3, 300, 300).to(device) # Larger image\nstart_im = im.clone() # Keep a copy of the initial noise image\nim.requires_grad = True\n\n# Our new and improved loss function\ndef clip_loss(image_features, text_features):\n    input_normed = torch.nn.functional.normalize(image_features.unsqueeze(1), dim=2)\n    embed_normed =torch.nn.functional.normalize(text_features.unsqueeze(0), dim=2)\n    dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2) # Squared Great Circle Distance\n    return dists.mean()\n\ntfms = torchvision.transforms.Compose([\n    torchvision.transforms.RandomResizedCrop(224), # Random CROP each time\n    torchvision.transforms.RandomAffine(5),\n    # torchvision.transforms.ColorJitter(), # You can experiment with different ones here\n    # torchvision.transforms.GaussianBlur(5), \n    torchvision.transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n])\n\nopt = torch.optim.Adam([im], lr=5e-3)\nlosses = []\n\nfor i in tqdm(range(350)):\n    image_features = model.encode_image(tfms(im))\n    loss = clip_loss(image_features, text_features)\n    loss.backward()\n    losses.append(loss.item())\n    opt.step()\n    opt.zero_grad()\n    \ntensor_to_pil(im)\n\n\n\n\n\n\n\nBetter but still not amazing! TODO talk about more possible improvementsfrom tglcourse.generation_utils import SirenGenerator, CLIPLossToTargets, optimise\n\nfrom tglcourse.generation_utils import ImStackGenerator, CLIPLossToTargets, optimise\n\n# Parameters of the imstack tweaked\ngen = ImStackGenerator(size=300,base_size=8,n_layers=3,scale=3,layer_decay = 0.3).to(device)\n\n# A custom optimiser\nopt = torch.optim.AdamW(gen.parameters(), lr=0.05, \n                       weight_decay=1e-4) # Weight decay for less extreme values\n\n# Loss function\nclip_loss_fn = CLIPLossToTargets(text_prompts=['A watercolor painting of a frog on a lillypad'],\n                                 n_cuts=64) # More cuts for smoother loss signal\n\n# Tweak number of steps and use our custom optimiser\noptimise(gen, loss_functions=[clip_loss_fn],\n         optimizer=opt, n_steps=60)\n\n\n\n\n\n\n\n\n\n\n\n\nTODO video looking at Remi’s multi-perceptor notebook.\nPage stats: Total Hits:  Page visitors:"
  },
  {
    "objectID": "datasets.html",
    "href": "datasets.html",
    "title": "Datasets and General Data Utilities",
    "section": "",
    "text": "For many tasks in machine learning, the actual model training is the easy bit! Many data scientists spend most of their time sourcing and exploring data, and getting it into the right format ready for modelling. Lucky for us, most of the data we’ll use for demos during this course has already been collected and organised for us, and to make things even more convenient during the lessons themselves we’re going to lay some additional groundwork here in this notebook.\nMotivate dataloaders Batching Advantage of pre-fetching the next batch Mention monitoring GPU usage and watching for CPU bottlenecks in the dataloaders Dive into pytorch dataloaders HF Hub and datasets library\nhttps://huggingface.co/docs/datasets/quickstart\nhttps://huggingface.co/docs/datasets/stream\nDATA UTILS\n\nTODO redo this and integrate into notebooks\n\ndataloader = get_cifar10_dl(batch_size=128, split='train')\nbatch = next(iter(dataloader))\nbatch['image'].shape, batch['label']\n\nReusing dataset cifar10 (/root/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4)\nLoading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4/cache-16b9e105e7ead8c5.arrow\nParameter 'transform'=<function cifar10_transform> of the transform datasets.arrow_dataset.Dataset.set_format couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n\n\n(torch.Size([128, 3, 32, 32]),\n tensor([1, 2, 6, 7, 9, 4, 7, 6, 4, 2, 2, 0, 4, 8, 4, 2, 5, 7, 2, 9, 9, 8, 8, 1,\n         4, 3, 7, 3, 5, 6, 9, 3, 6, 4, 3, 4, 7, 9, 3, 3, 0, 6, 4, 3, 5, 1, 9, 6,\n         2, 2, 1, 0, 6, 7, 4, 3, 1, 4, 4, 2, 2, 5, 4, 5, 7, 0, 3, 0, 8, 4, 5, 7,\n         9, 0, 9, 9, 9, 4, 8, 3, 3, 6, 5, 5, 3, 2, 8, 1, 4, 3, 4, 2, 7, 8, 2, 0,\n         9, 6, 8, 7, 4, 3, 2, 0, 2, 0, 3, 2, 4, 9, 2, 5, 9, 6, 0, 6, 0, 7, 2, 2,\n         1, 7, 5, 9, 6, 8, 6, 4]))"
  },
  {
    "objectID": "scripts.html",
    "href": "scripts.html",
    "title": "Creating Scripts",
    "section": "",
    "text": "Notebooks are great for exploration. And with NBDev they’re pretty amazing for development work too! But sometimes you just want a nice command-line script so that you can run your code with some thing like:\npython my_script.py --input cat.png --width 128 \nIn this quick notebook I’ll show you my new favourite way of doing this, using fastcore.script. The example from their docs shows how simple this can be. We write the following to a file:\n\nfrom fastcore.script import *\n@call_parse\ndef main(msg:str,     # The message\n         upper:bool): # Convert to uppercase?\n    \"Print `msg`, optionally converting to uppercase\"\n    print(msg.upper() if upper else msg)\n\nOverwriting scripts/example_script.py\n\n\nNow we can run this like so:\n\n!python scripts/example_script.py --upper 'Hello World'\n\nHELLO WORLD\n\n\nAnd those type hints and comments in the function definition above? They become part of the help text:\n\n!python scripts/example_script.py -h\n\nusage: example_script.py [-h] [--upper] msg\n\nPrint `msg`, optionally converting to uppercase\n\npositional arguments:\n  msg         The message\n\noptions:\n  -h, --help  show this help message and exit\n  --upper     Convert to uppercase? (default: False)\n\n\nPretty nifty right!\nCheck out the scripts/ directory for a bunch of examples which I made using this technique to go along with the course."
  },
  {
    "objectID": "dm3.html",
    "href": "dm3.html",
    "title": "Stable Diffusion",
    "section": "",
    "text": "This is based on Unit 3 of the Hugging Face Diffusion Models Course. In this unit you will meet a powerful diffusion model called Stable Diffusion (SD) and explore what it can do."
  },
  {
    "objectID": "dm3.html#get-started",
    "href": "dm3.html#get-started",
    "title": "Stable Diffusion",
    "section": "Get Started",
    "text": "Get Started\nHere are the steps for this unit:\n\nRead through the material below for an overview of the key ideas of this unit\nCheck out the Stable Diffusion Introduction notebook to see SD applied in practice to some common use-cases\nCheck out the Stable Diffusion Deep Dive video (below) and the accompanying notebook for a deeper exploration of the different components and how they can be adapted for different effects. This material was created for the new FastAI course, Stable Diffusion from the Foundations - the first few lessons are already available and the rest will be released in the next few months, making this a great supplement to this class for anyone curious about building these kinds of models completely from scratch.\nEnter the HuggingFace Dreambooth Hackathon for extra fun and a chance to win a bit of swag"
  },
  {
    "objectID": "dm3.html#introduction",
    "href": "dm3.html#introduction",
    "title": "Stable Diffusion",
    "section": "Introduction",
    "text": "Introduction\n Example images generated using Stable Diffusion\nStable Diffusion is a powerful text-conditioned latent diffusion model. Don’t worry, we’ll explain those words shortly! Its ability to create amazing images from text descriptions has made it an internet sensation. In this unit, we’re going to explore how SD works and see what other tricks it can do."
  },
  {
    "objectID": "dm3.html#latent-diffusion",
    "href": "dm3.html#latent-diffusion",
    "title": "Stable Diffusion",
    "section": "Latent Diffusion",
    "text": "Latent Diffusion\nAs image size grows, so does the computational power required to work with those images. This is especially pronounced in an operation called self-attention, where the amount of operations grows quadratically with the number of inputs. A 128px square image has 4x as many pixels as a 64px square image, and so requires 16x (i.e. 42) the memory and compute in a self-attention layer. This is a problem for anyone who’d like to generate high-resolution images!\n Diagram from the Latent Diffusion paper\nLatent diffusion helps to mitigate this issue by using a separate model called a Variational Auto-Encoder (VAE) to compress images to a smaller spatial dimension. The rationale behind this is that images tend to contain a large amount of redundant information - given enough training data, a VAE can hopefully learn to produce a much smaller representation of an input image and then reconstruct the image based on this small latent representation with a high degree of fidelity. The VAE used in SD takes in 3-channel images and produces a 4-channel latent representation with a reduction factor of 8 for each spatial dimension. That is, a 512px square input image will be compressed down to a 4x64x64 latent.\nBy applying the diffusion process on these latent representations rather than on full-resolution images, we can get many of the benefits that would come from using smaller images (lower memory usage, fewer layers needed in the UNet, faster generation times…) and still decode the result back to a high-resolution image once we’re ready to view the final result. This innovation dramatically lowers the cost to train and run these models."
  },
  {
    "objectID": "dm3.html#text-conditioning",
    "href": "dm3.html#text-conditioning",
    "title": "Stable Diffusion",
    "section": "Text Conditioning",
    "text": "Text Conditioning\nIn Unit 2 we showed how feeding additional information to the UNet allows us to have some additional control over the types of images generated. We call this conditioning. Given a noisy version of an image, the model is tasked with predicting the denoised version based on additional clues such as a class label or, in the case of Stable Diffusion, a text description of the image. At inference time, we can feed in the description of an image we’d like to see and some pure noise as a starting point, and the model does its best to ‘denoise’ the random input into something that matches the caption.\n Diagram showing the text encoding process which transforms the input prompt into a set of text embeddings (the encoder_hidden_states) which can then be fed in as conditioning to the UNet.\nFor this to work, we need to create a numeric representation of the text that captures relevant information about what it describes. To do this, SD leverages a pre-trained transformer model based on something called CLIP. CLIP’s text encoder was designed to process image captions into a form that could be used to compare images and text, so it is well suited to the task of creating useful representations from image descriptions. An input prompt is first tokenized (based on a large vocabulary where each word or sub-word is assigned a specific token) and then fed through the CLIP text encoder, producing a 768-dimensional (in the case of SD 1.X) or 1024-dimensional (SD 2.X) vector for each token. To keep things consistent prompts are always padded/truncated to be 77 tokens long, and so the final representation which we use as conditioning is a tensor of shape 77x1024 per prompt.\n\n\n\nconditioning diagram\n\n\nOK, so how do we actually feed this conditioning information into the UNet for it to use as it makes predictions? The answer is something called cross-attention. Scattered throughout the UNet are cross-attention layers. Each spatial location in the UNet can ‘attend’ to different tokens in the text conditioning, bringing in relevant information from the prompt. The diagram above shows how this text conditioning (as well as timestep-based conditioning) is fed in at different points. As you can see, at every level the UNet has ample opportunity to make use of this conditioning!"
  },
  {
    "objectID": "dm3.html#classifier-free-guidance",
    "href": "dm3.html#classifier-free-guidance",
    "title": "Stable Diffusion",
    "section": "Classifier-free Guidance",
    "text": "Classifier-free Guidance\nIt turns out that even with all of the effort put into making the text conditioning as useful as possible, the model still tends to default to relying mostly on the noisy input image rather than the prompt when making its predictions. In a way, this makes sense - many captions are only loosely related to their associated images and so the model learns not to rely too heavily on the descriptions! However, this is undesirable when it comes time to generate new images - if the model doesn’t follow the prompt then we may get images out that don’t relate to our description at all.\n Images generated from the prompt “An oil painting of a collie in a top hat” with CFG scale 0, 1, 2 and 10 (left to right)\nTo fix this, we use a trick called Classifier-Free Guidance (CGF). During training, text conditioning is sometimes kept blank, forcing the model to learn to denoise images with no text information whatsoever (unconditional generation). Then at inference time, we make two separate predictions: one with the text prompt as conditioning and one without. We can then use the difference between these two predictions to create a final combined prediction that pushes even further in the direction indicated by the text-conditioned prediction according to some scaling factor (the guidance scale), hopefully resulting in an image that better matches the prompt. The image above shows the outputs for a prompt at different guidance scales - as you can see, higher values result in images that better match the description."
  },
  {
    "objectID": "dm3.html#other-types-of-conditioning-super-resolution-inpainting-and-depth-to-image",
    "href": "dm3.html#other-types-of-conditioning-super-resolution-inpainting-and-depth-to-image",
    "title": "Stable Diffusion",
    "section": "Other Types of Conditioning: Super-Resolution, Inpainting and Depth-to-Image",
    "text": "Other Types of Conditioning: Super-Resolution, Inpainting and Depth-to-Image\nIt is possible to create versions of Stable Diffusion that take in additional kinds of conditioning. For example, the Depth-to-Image model has extra input channels that take in-depth information about the image being denoised, and at inference time we can feed in the depth map of a target image (estimated using a separate model) to hopefully generate an image with a similar overall structure.\n Depth-conditioned SD is able to generate different images with the same overall structure (example from StabilityAI)\nIn a similar manner, we can feed in a low-resolution image as the conditioning and have the model generate the high-resolution version (as used by the Stable Diffusion Upscaler). Finally, we can feed in a mask showing a region of the image to be re-generated as part of the ‘in-painting’ task, where the non-mask regions need to stay intact while new content is generated for the masked area."
  },
  {
    "objectID": "dm3.html#fine-tuning-with-dreambooth",
    "href": "dm3.html#fine-tuning-with-dreambooth",
    "title": "Stable Diffusion",
    "section": "Fine-Tuning with DreamBooth",
    "text": "Fine-Tuning with DreamBooth\n Image from the dreambooth project page based on the Imagen model\nDreamBooth is a technique for fine-tuning a text-to-image model to ‘teach’ it a new concept, such as a specific object or style. The technique was originally developed for Google’s Imagen model but was quickly adapted to work for stable diffusion. Results can be extremely impressive (if you’ve seen anyone with an AI profile picture on social media recently the odds are high it came from a dreambooth-based service) but the technique is also sensitive to the settings used, so check out our notebook and this great investigation into the different training parameters for some tips on getting it working as well as possible."
  },
  {
    "objectID": "dm3.html#hands-on-notebook",
    "href": "dm3.html#hands-on-notebook",
    "title": "Stable Diffusion",
    "section": "Hands-On Notebook",
    "text": "Hands-On Notebook\n\n\n\nChapter\nColab\nKaggle\nGradient\nStudio Lab\n\n\n\n\nStable Diffusion Introduction\n\n\n\n\n\n\nDreamBooth Hackathon Notebook\n\n\n\n\n\n\nStable Diffusion Deep Dive\n\n\n\n\n\n\n\nAt this point, you know enough to get started with the accompanying notebooks! Open them in your platform of choice using the links above. Dreambooth requires quite a lot of compute power, so if you’re using Kaggle or Google Colab make sure you set the runtime type to ‘GPU’ for the best results.\nThe ‘Stable Diffusion Introduction’ notebook is a short introduction to stable diffusion with the 🤗 Diffusers library, stepping through some basic usage examples using pipelines to generate and modify images.\nIn the DreamBooth Hackathon Notebook (in the hackathon folder) we show how you can fine-tune SD on your own images to create a custom version of the model covering a new style or concept.\nFinally, the ‘Stable Diffusion Deep Dive’ notebook and video break down every step in a typical generation pipeline, suggesting some novel ways to modify each stage for additional creative control."
  },
  {
    "objectID": "dm3.html#project-time",
    "href": "dm3.html#project-time",
    "title": "Stable Diffusion",
    "section": "Project Time",
    "text": "Project Time\nFollow the instructions in the DreamBooth notebook to train your own model for one of the specified categories. Make sure you include the example outputs in your submission so that we can choose the best models in each category! See the hackathon info for details on prizes, GPU credits and more."
  },
  {
    "objectID": "dm3.html#some-additional-resources",
    "href": "dm3.html#some-additional-resources",
    "title": "Stable Diffusion",
    "section": "Some Additional Resources",
    "text": "Some Additional Resources\n\nHigh-Resolution Image Synthesis with Latent Diffusion Models - The paper that introduced the approach behind Stable Diffusion\nCLIP - CLIP learns to connect text with images and the CLIP text encoder is used to transform a text prompt into the rich numerical representation used by SD. See also, this article on OpenCLIP for some background on recent open-source CLIP variants (one of which is used for SD version 2).\nGLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models an early paper demonstrating text conditioning and CFG\n\nFound more great resources? Let us know and we’ll add them to this list."
  },
  {
    "objectID": "transformers.html",
    "href": "transformers.html",
    "title": "Lesson 10 - Transformers",
    "section": "",
    "text": "# https://youtube.com/watch?v=gDNRnjcoMOY\n\nIllustrated transformer: https://jalammar.github.io/illustrated-transformer/\nPage stats: Total Hits:  Page visitors:"
  },
  {
    "objectID": "generative_1.html",
    "href": "generative_1.html",
    "title": "Lesson 6 - Generative Modelling Intro (AutoEncoders)",
    "section": "",
    "text": "So far in this course we"
  },
  {
    "objectID": "generative_1.html#auto-encoders-learned-representations-and-vaes",
    "href": "generative_1.html#auto-encoders-learned-representations-and-vaes",
    "title": "Lesson 6 - Generative Modelling Intro (AutoEncoders)",
    "section": "Auto-Encoders, Learned Representations and VAEs",
    "text": "Auto-Encoders, Learned Representations and VAEs\nWe’ve seen that pre-trained networks can learn representations that are useful for classification. And that we can use some of these learned representations to create a measure of structural or stylistic similarity between images, which we can re-purpose for style transfer.\nHow does this idea of learning useful representations tie into generating new images?\nIn this section we’ll meet something called an autoencoder.\n\n\n\n\n\nThe idea here is to make a neural network with two parts: and encoder and a decoder. The encoder will take an image and process it until, at some point, we reach a ‘bottleneck’ layer where only a handful of numbers are passed to the next layer. We call this small set of numbers h or z* or ‘the latent representation of the image’. The task of the network during training is to learn how to produce a useful representation so that the decoder can re-create the input image with minimal loss.\nBy setting things up in this way, we’re forcing the network to learn enough about the data to create a useful representation AND a decoder network that can create nice outputs despite only seeing this very compressed representation of the input image.\nTake a moment to appreciate this: as with many things in deep learning, we don’t do anything too special ourselves - we just set an objective and let the network learn how to do things!\n*some people are picky about notation, but we’ll be fairly flexible and just try to clarify what something is rather than relying too heavily on one poor letter for meaning.\n\nCreating a Convolutional AutoEncoder\nHere we build on the simple networks we made in lesson 3 to create our network. Skim the code and see if you can understand how the encoder and decoder work. Note that ConvTranspose2d is effectively the reverse of a convolution layer - check the docs for details.\n\nclass ConvAE(nn.Module):\n    def __init__(self, hdim=20):\n        super(ConvAE, self).__init__()\n        # Encoder layers\n        self.enc_conv1 = nn.Conv2d(1, 32, 3)\n        self.enc_conv2 = nn.Conv2d(32, 32, 3)\n        self.enc_fc1 = nn.Linear(32*24*24, 128)\n        self.enc_fc2 = nn.Linear(128, hdim)\n\n        # Decoder layers\n        self.dec_fc1 = nn.Linear(hdim, 128)\n        self.dec_fc2 = nn.Linear(128, 32*24*24)\n        self.dec_unflatten = nn.Unflatten(dim=-1, unflattened_size=(32, 24, 24))\n        self.dec_conv1 = nn.ConvTranspose2d(32, 32, 3)\n        self.dec_conv2 = nn.ConvTranspose2d(32, 1, 3)\n\n    def encode(self, x):\n        x = self.enc_conv1(x)\n        x = F.relu(x)\n        x = self.enc_conv2(x)\n        x = F.relu(x)\n        x = torch.flatten(x, 1)\n        x = self.enc_fc1(x)\n        x = F.relu(x)\n        x = self.enc_fc2(x)\n        return x\n\n    def decode(self, x):\n        x = self.dec_fc1(x)\n        x = F.relu(x)\n        x = self.dec_fc2(x)\n        x = F.relu(x)\n        x = self.dec_unflatten(x)\n        x = self.dec_conv1(x)\n        x = F.relu(x)\n        x = self.dec_conv2(x)\n        return torch.sigmoid(x) # Sigmoid means the output must be between 0 and 1 (you can explore removing this)\n\n    def forward(self, x):\n        return self.decode(self.encode(x))"
  },
  {
    "objectID": "generative_1.html#training-it-on-some-data",
    "href": "generative_1.html#training-it-on-some-data",
    "title": "Lesson 6 - Generative Modelling Intro (AutoEncoders)",
    "section": "Training it on some data",
    "text": "Training it on some data\n\nmnist = datasets.MNIST('./mnist_data/',train=True,transform=transforms.ToTensor(), download=True)\nprint('Example image:')\nplt.imshow(mnist[0][0].squeeze(), cmap='gray');\n\nExample image:\n\n\n\n\n\n\n# Create a network as defined above and pass an image through it\nae = ConvAE()\nx = mnist[0][0].unsqueeze(0)\nprint('Input shape:', x.shape)\nencoded = ae.encode(x)\nprint('Encoded shape:', encoded.shape)\ndecoded = ae.decode(encoded)\nprint('Decoded shape:', decoded.shape)\nprint('Decoded image:')\nplt.imshow(decoded.detach().squeeze(), cmap='gray')\n\nInput shape: torch.Size([1, 1, 28, 28])\nEncoded shape: torch.Size([1, 20])\nDecoded shape: torch.Size([1, 1, 28, 28])\nDecoded image:\n\n\n<matplotlib.image.AxesImage>\n\n\n\n\n\nTime to train it. I stole most of this code from the NMA content on this topic, which is where I’d suggest you start if you want to go deeper.\n\nae.to(device)\nbatch_size=128\noptim = torch.optim.Adam(ae.parameters(),lr=1e-3,weight_decay=1e-5)\nloss_fn = nn.MSELoss()\ndataloader = DataLoader(mnist,batch_size=batch_size,shuffle=True, drop_last=True)\nfor epoch in range(2):\n    for im_batch, _ in tqdm(dataloader):\n        im_batch = im_batch.to(device)\n        optim.zero_grad()\n        reconstruction = ae(im_batch) # The forward method encodes and then decodes\n        loss = loss_fn(reconstruction.view(batch_size, -1),\n                       target=im_batch.view(batch_size, -1))\n        loss.backward()\n        optim.step()\n\n\n\n\n\n\n\nAfter training, we no longer get the noisy nothingness we saw as the output of the network. Instead, the images look impressively close to the originals! You could try lowering the size of the hidden dimension and seeing how far you can push it.\n\n# Plot the reconstructed versions of some random samples (ideally we'd use examles from a validation set not used for training)\nfig, axs = plt.subplots(2, 4, figsize=(12, 6))\nfor i in range(4):\n    im, label = mnist[random.randint(0, len(mnist))]\n    axs[0][i].imshow(im.squeeze(), cmap='gray')\n    axs[0][i].set_title(label)\n    axs[1][i].imshow(ae(im.unsqueeze(0).to(device)).squeeze().detach().cpu(), cmap='gray')\n    axs[1][i].set_title(str(label) + ' - reconstructed')\n\n\n\n\nAutoencoders are great, and have many uses (data compression, for example). But they aren’t so great at producing new images that look like the training data. Ideally, we’d pick a random point in the latent space (i.e. a set of random numbers to be the h) and run it through the decoder to get something that looks like a real image. Instead, if we try this we see mostly strange alien shapes:\n\n# Generate new images from random zs\nfig, axs = plt.subplots(2, 4, figsize=(12, 6))\nfor i in range(8):\n    im = ae.decode(torch.randn(1, 20).to(device))\n    axs[i//4][i%4].imshow(im.detach().cpu().squeeze(), cmap='gray')\n\n\n\n\nThis is because the autoencoder learns to map different image classes to drastically different parts of the latent space - there is no incentive for it to learn a ‘neat’ representation, only that it learns one that let’s it solve the problem as we framed it.\nTo ‘fix’ this, we need to add an additional component to the loss which stops it from keeping all the latent representations wildly separated."
  },
  {
    "objectID": "generative_1.html#variational-autoencoders",
    "href": "generative_1.html#variational-autoencoders",
    "title": "Lesson 6 - Generative Modelling Intro (AutoEncoders)",
    "section": "Variational AutoEncoders",
    "text": "Variational AutoEncoders\nGreat explanation: https://www.youtube.com/watch?v=9zKuYvjFFS8\nThe architecture will be almost identical to the plain AE above, but now our encoder will have two outputs to represent a distribution in the latent space: one for the mean, and one for the standard deviation (or rather, the log of the variance which we can turn into a standard deviation with torch.exp(0.5*logvar)). To reconstruct an image, we sample from this distribution to get the hidden representation z which is fed to the decoder, which is unchanged from the previous example.\nOne subtlety here: reparametrization trick\n\nclass ConvVAE(nn.Module):\n    def __init__(self, hdim=20):\n        super(ConvVAE, self).__init__()\n        # Encoder layers\n        self.enc_conv1 = nn.Conv2d(1, 32, 3)\n        self.enc_conv2 = nn.Conv2d(32, 32, 3)\n        self.enc_fc1 = nn.Linear(32*24*24, 128)\n        self.enc_fc2_mean = nn.Linear(128, hdim)\n        self.enc_fc2_logvar = nn.Linear(128, hdim)\n\n        # Decoder layers\n        self.dec_fc1 = nn.Linear(hdim, 128)\n        self.dec_fc2 = nn.Linear(128, 32*24*24)\n        self.dec_unflatten = nn.Unflatten(dim=-1, unflattened_size=(32, 24, 24))\n        self.dec_conv1 = nn.ConvTranspose2d(32, 32, 3)\n        self.dec_conv2 = nn.ConvTranspose2d(32, 1, 3)\n\n    def encode(self, x):\n        x = self.enc_conv1(x)\n        x = F.relu(x)\n        x = self.enc_conv2(x)\n        x = F.relu(x)\n        x = torch.flatten(x, 1)\n        x = self.enc_fc1(x)\n        x = F.relu(x)\n        mean = self.enc_fc2_mean(x)\n        logvar = self.enc_fc2_logvar(x)\n        return mean, logvar\n    \n    def reparameterize(self, mean, logvar):\n        std = torch.exp(0.5*logvar) # 0.5*logvar in the exponent is being fancy. Can do std = torch.sqrt(torch.exp(logvar))\n        eps = torch.randn_like(std) # This isn't dependant on mean or logvar\n        return mean + eps*std # So we can trace gradients back through this operation\n\n    def decode(self, x):\n        x = self.dec_fc1(x)\n        x = F.relu(x)\n        x = self.dec_fc2(x)\n        x = F.relu(x)\n        x = self.dec_unflatten(x)\n        x = self.dec_conv1(x)\n        x = F.relu(x)\n        x = self.dec_conv2(x)\n        return torch.sigmoid(x) # Clamps value between 0 and 1\n\n    def forward(self, x, return_dist = False):\n        mean, logvar = self.encode(x) # Predict a distribution\n        z = self.reparameterize(mean, logvar) # Sample from it\n        if return_dist: return self.decode(z), mean, logvar \n        return self.decode(z)\n\n\n# Some code from https://github.com/pytorch/examples/blob/main/vae/main.py\n\n\nvae = ConvVAE()\nvae.to(device)\nbatch_size=128\noptim = torch.optim.Adam(vae.parameters(),lr=1e-3,weight_decay=1e-5)\nloss_fn = nn.MSELoss()\ndataloader = DataLoader(mnist,batch_size=batch_size,shuffle=True, drop_last=True)\nlosses = []\nfor epoch in range(3):\n    for im_batch, _ in tqdm(dataloader):\n        im_batch = im_batch.to(device)\n        optim.zero_grad()\n        \n        # Get the reconstruction as well as the predicted distribution (mean and logvar):\n        reconstruction, mean, logvar = vae(im_batch, return_dist=True) \n            \n        # Reconstruction loss\n        BCE = F.binary_cross_entropy(reconstruction, im_batch, reduction='sum')\n        \n        # KL Divergence\n        KLD = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())\n        \n        # Combine the two to get final loss\n        loss = BCE + KLD\n        losses.append(loss.item())\n        loss.backward()\n        optim.step()\nplt.plot(losses)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Generate new images from random zs\nfig, axs = plt.subplots(2, 4, figsize=(12, 6))\nfor i in range(8):\n    im = vae.decode(torch.randn(1, 20).to(device)) # Random point\n    axs[i//4][i%4].imshow(im.detach().cpu().squeeze(), cmap='gray')\n\n\n\n\nAs you can see, these outputs look much more like digits than the non-variational equivalent! We have successfully trained a model that can turn a random noise input into a plausible looking image.\nExercise: Try some experiments based on the code above. For example:  - What happens if you use a low hidden dimension size (say, 5, or 2) - can the model still reconstruct images well? How diverse are the generated images? - Can you make a linear version (no convolutions) of the VAE above? You’ll need to flatten the input images and think about how many inputs and outputs each layer should have (the official PyTorch vae example is a good reference if you get stuck) - How good can you get? Use the test set of MNIST and measure reconstruction loss (with something like MSE between the inages and the encoded-then-decoded versions) or generate a bunch of examples and visually assess them. - Try this o a dataset of color images - perhaps faces. Do any issues stand out that aren’t apparent with a simple dataset like MNIST?\n\n# Your code here\n\nPage stats: Total Hits:  Page visitors:"
  }
]